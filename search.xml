<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Natural Language Inference 学习笔记]]></title>
    <url>%2F2018%2F08%2F06%2FNatural-Language-Inference-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[讲讲最近的实习经历，也算是如愿半路出家跳入 NLP 坑里来了。 背景介绍来到携程这边后 leader 丢给我三个选项：游记与产品关联并推荐、矛盾条款分析 以及 KBQA。各方面考虑再三后，我选择了矛盾条款分析，实际上也就是 NLI 任务的简化。 矛盾条款分析是属于公司一个叫照妖镜项目下的子任务。相关商家可以把自家的产品发布到携程相关的产品页面上来，照妖镜的功能就是实时地去检查商家发布的产品是否合规，是否有效，如果产品有问题的话 (违规、失去时效) 就将其自动下架甚至对商家进行惩罚。 而矛盾条款分析就是针对商家发布的产品的参与规则进行分析，看其条款是否互相矛盾。举个例子，比如商品参与规则中的其中一条写着“本产品一价全包，后续无需其它费用”，而另一条则写着“本产品不包含 xx 景点门票费用，如欲游览需要自行买票”。那这样这两条条款就是互相矛盾的了，需要模型判断出来并对商家进行惩罚或者下架。 NLI 任务与研究进展以上所描述的矛盾条款分析任务其实就是 NLI 文本蕴含 (Text Entailment) 任务的简化，它的任务形式为：给定一个前提 (Premise) ，根据这个前提去推断假设 (Hypothesis) 与前提的关系。该任务的关系分为三种，蕴含关系 (Entailment)、矛盾关系 (Contradiction) 以及中立关系 (Neutral)。所以这个问题本质上是一个分类问题，我们需要做的是去发掘前提和假设这两个句子对之间的交互信息。 NLI 任务比较常用的标准数据集为 SNLI (The Stanford Natural Language Inference corpus) 和 MultiNLI (The Multi-Genre Natural Language Inference corpus)。后来 Kaggle 上面又出了一个很出名的比赛，Quora Question Pairs，它做的只是判断 Quora 上的两个问题句是否表示的是一样的意思。 个人而言，我很喜欢 SNLI 这个数据集，并不是因为它最早出现，而是其网站上列出了当前的研究进展、之前在这个任务上的一些经典做法。跟着这张表去刷论文能够让你的学习效率提高很多，反正对于我这种半路出家的人而言，这张表对我的帮助非常大，在接下来写的内容也大部分来自这张表所提及的文章中。 Publication&nbsp;ModelParameters&nbsp;Train (% acc)&nbsp;Test (% acc)Feature-based modelsBowman et al. ‘15Unlexicalized features49.450.4Bowman et al. ‘15+ Unigram and bigram features99.778.2Sentence encoding-based modelsBowman et al. ‘15100D LSTM encoders220k84.877.6Bowman et al. ‘16300D LSTM encoders3.0m83.980.6Vendrov et al. ‘151024D GRU encoders w/ unsupervised ‘skip-thoughts’ pre-training15m98.881.4Mou et al. ‘15300D Tree-based CNN encoders3.5m83.382.1Bowman et al. ‘16300D SPINN-PI encoders3.7m89.283.2Yang Liu et al. ‘16600D (300+300) BiLSTM encoders2.0m86.483.3Munkhdalai &amp; Yu ‘16b300D NTI-SLSTM-LSTM encoders4.0m82.583.4Yang Liu et al. ‘16600D (300+300) BiLSTM encoders with intra-attention2.8m84.584.2Conneau et al. ‘174096D BiLSTM with max-pooling40m85.684.5Munkhdalai &amp; Yu ‘16a300D NSE encoders3.0m86.284.6Qian Chen et al. ‘17600D (300+300) Deep Gated Attn. BiLSTM encoders (code)12m90.585.5Tao Shen et al. ‘17300D Directional self-attention network encoders (code)2.4m91.185.6Jihun Choi et al. ‘17300D Gumbel TreeLSTM encoders2.9m91.285.6Nie and Bansal ‘17300D Residual stacked encoders9.7m89.885.7Anonymous ‘181200D REGMAPR (Base+Reg)––85.9Yi Tay et al. ‘18300D CAFE (no cross-sentence attention)3.7m87.3 85.9Jihun Choi et al. ‘17600D Gumbel TreeLSTM encoders10m93.186.0Nie and Bansal ‘17600D Residual stacked encoders29m91.086.0Tao Shen et al. ‘18300D Reinforced Self-Attention Network3.1m92.686.3Im and Cho ‘17Distance-based Self-Attention Network4.7m89.686.3Seonhoon Kim et al. ‘18Densely-Connected Recurrent and Co-Attentive Network (encoder)5.6m91.486.5Qian Chen et al. ‘18600D BiLSTM with generalized pooling65m94.986.6Other neural network modelsRocktäschel et al. ‘15100D LSTMs w/ word-by-word attention250k85.383.5Pengfei Liu et al. ‘16a100D DF-LSTM320k85.284.6Yang Liu et al. ‘16600D (300+300) BiLSTM encoders with intra-attention and symbolic preproc.2.8m85.985.0Pengfei Liu et al. ‘16b50D stacked TC-LSTMs190k86.785.1Munkhdalai &amp; Yu ‘16a300D MMA-NSE encoders with attention3.2m86.985.4Wang &amp; Jiang ‘15300D mLSTM word-by-word attention model1.9m92.086.1Jianpeng Cheng et al. ‘16300D LSTMN with deep attention fusion1.7m87.385.7Jianpeng Cheng et al. ‘16450D LSTMN with deep attention fusion3.4m88.586.3Parikh et al. ‘16200D decomposable attention model380k89.586.3Parikh et al. ‘16200D decomposable attention model with intra-sentence attention580k90.586.8Munkhdalai &amp; Yu ‘16b300D Full tree matching NTI-SLSTM-LSTM w/ global attention3.2m88.587.3Zhiguo Wang et al. ‘17BiMPM1.6m90.9 87.5Lei Sha et al. ‘16300D re-read LSTM2.0m90.787.5Yichen Gong et al. ‘17448D Densely Interactive Inference Network (DIIN, code)4.4m91.288.0McCann et al. ‘17Biattentive Classification Network + CoVe + Char22m88.588.1Chuanqi Tan et al. ‘18150D Multiway Attention Network14m94.5 88.3Xiaodong Liu et al. ‘18Stochastic Answer Network3.5m93.388.5Ghaeini et al. ‘18450D DR-BiLSTM7.5m94.1 88.5Yi Tay et al. ‘18300D CAFE4.7m89.8 88.5Qian Chen et al. ‘17KIM4.3m94.1 88.6Qian Chen et al. ‘16600D ESIM + 300D Syntactic TreeLSTM (code)7.7m93.5 88.6Peters et al. ‘18ESIM + ELMo8.0m91.6 88.7Boyuan Pan et al. ‘18300D DMAN9.2m95.4 88.8Zhiguo Wang et al. ‘17BiMPM Ensemble6.4m93.2 88.8Yichen Gong et al. ‘17448D Densely Interactive Inference Network (DIIN, code) Ensemble17m92.3 88.9Seonhoon Kim et al. ‘18Densely-Connected Recurrent and Co-Attentive Network6.7m93.188.9Qian Chen et al. ‘17KIM Ensemble43m93.6 89.1Ghaeini et al. ‘18450D DR-BiLSTM Ensemble45m94.8 89.3Peters et al. ‘18ESIM + ELMo Ensemble40m92.1 89.3Yi Tay et al. ‘18300D CAFE Ensemble17.5m92.589.3Chuanqi Tan et al. ‘18150D Multiway Attention Network Ensemble58m95.5 89.4Boyuan Pan et al. ‘18300D DMAN Ensemble79m96.1 89.6Radford et al. ‘18Fine-Tuned LM-Pretrained Transformer85m96.689.9Seonhoon Kim et al. ‘18Densely-Connected Recurrent and Co-Attentive Network Ensemble53.3m95.090.1 模型架构由上图可以看出，NLI 任务的模型被分成了三类，Feature-based models, Sentence encoding-based models, Other neural network models。下面的内容就是从上面的内容中挑出来的几篇比较经典的文章。（说这么好听，其实也就是我最近做矛盾条款这个项目刷过的论文，逃） Feature-based models表中的 Feature-based 的方法其实就只涉及了一篇文章，A large annotated corpus for learning natural language inference。这篇文章发表在了 EMNLP2015，也正是这篇文章公开了 SNLI 这个标准数据集。文中其实对 Feature-based 和 Sentence encoding-based 的方法都有介绍，由于小节标题的缘故，在这个部分就只介绍一下他提取的特征吧。 文中使用了 6 种特征，3 种是 unlexicalized，另外 3 种则是 lexicalized。 Unlexicalized 的特征包括了： 计算 1-4 gram 的前提 (premise, 后文简写为 $p$) 和假设 (hypothesis, 后文简写为 $h$) 的 BLEU 分数 $p$ 和 $h$ 的长度差 $p$ 和 $h$ 的重复词的频率、概率，甚至还统计到了名词、动词、形容词和副词的层面上。 Lexicalized 的特征包括了： 统计了 $h$ 中 unigram 和 bigram 的指标 统计了 $p$ 和 $h$ 中共有的 POS 信息，包含 unigram 和 bigram 的。 作者还认为更复杂的模型可能会有更好的效果，于是就从 Sentence encoding-based 的思想出发，又做了一组实验。 Sentence encoding-based modelsLSTM encoders接着上一节的特征工程，Bowman 等人用 LSTM 来分别对 $h$ 和 $p$ 进行编码，之后再将输出丢到三层的带 tanh 激活函数的全连接层中，最后过一下 softmax 得到最终结果。模型架构很简单，毕竟这是 15 年的文章了，但还是有必要 show 一下，毕竟这是后续模型的雏形。 Tree-based CNN encoders本来是不打算讲不同 encoder 的文章的 (比如 BiLSTM, GRU 之类的 encoder)，不过这篇发表在 ACL2016 的 short paper 上的 TBCNN 还是挺值得一提的，因为他的拼接方法比较有意思，直接上图。 注意这里红框内的拼接部分，除了对 $p$ 和 $h$ 做简单的拼接之外，还做了 $p-h$，$p\cdot h$ 的操作，作者给出的解释是： The latter two are certain measures of “similarity” or “closeness.” 于是最后拼接起来的向量为 $m = [p; h; p - h; p \cdot h]$。注意一下这个拼接方式，因为后续的很多模型最后都是用这种拼接方法的类似方法来做的，足以见得这种方法的有效性。 InferSent这算是插播一条 InferSent 的介绍吧，InferSent 应该是目前最好的有监督 sentence embedding 的方法，它的出现改变了人们一贯认为的在 embedding 研究中，无监督是优于有监督的陈旧观点。 InferSent 模型出自 FAIR 发表在 EMNLP2017 上的 Supervised Learning of Universal Sentence Representations from Natural Language Inference Data。作者选用了 NLI 任务来进行有监督的句向量训练，原因是因为 NLI 任务本身的特殊性，它是涉及了句子内语义关系推导的高级自然语言理解任务，所以以这个任务为目标进行训练，可以产生较高质量的句向量表示。原文是这么写的： We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences. InferSent 的模型图如下所示： 它的 sentence encoder 用的是 biLSTM，然后对 biLSTM 输出的隐藏状态按 timestep 轴做一个 max-pooling，理由是 biLSTM 这种双向 RNN 结构可以很好地结合上下文信息对单词进行再编码，之后的 max-pooling 则是保留了不同时刻中整个句子最重要的信息。然后就是上一小节所示的 $[u; v; |u - v|; u \cdot v]$ 的拼接形式 (注意这里使用的是绝对值求差，但本质上没什么区别，都是求前提和假设之间的线性相似度) ，最后接一下全连接层和 softmax。然后最终得到的 $u$ 和 $v$ 就可以作为该句子的句向量表示来使用在其他任务上了。模型很简单，但是取得了很好的效果。 最后附上我个人用 TensorFlow 复现的 InferSent 代码。 Structured Self Attention讲了以上的 sentence embedding 的方法，这里顺道再讲一篇。 A Structured Self-Attentive Sentence Embedding 是发表在 ICLR2017 上的一篇文章，其本质上是对输入序列做 self-attention 操作，与普通 self-attention 不同的是，这篇文章的做法是将得到的 attention 权重扩展到了 $r$ 维，这样就得到了一个二维的句向量表示 $(r \times n)$。其实也就是将原本的 $(1 \times n)$ 的 attention 权重映射到了 $r$ 个不同的子空间上。 至于为什么要这么做，作者给出了解释：普通的 self-attention 会使得模型关注句子的一个特定的部分，也就是跟目标更加相关的单词或者短语的地方，希望通过突出这个部分来更好地表示这个句子的意思。然而，在很多情况下，尤其是遇到一个长句子的时候，句子中通常会有多个比较重要的部分，这时候你再只关注其中一块的话就有点不够用了，所以需要多个子空间才能够完整地表示整个句子的意思。 This vector representation usually focuses on a speciﬁc component of the sentence, like a special set of related words or phrases. So it is expected to reﬂect an aspect, or component of the semantics in a sentence. However, there can be multiple components in a sentence that together forms the overall semantics of the whole sentence, especially for long sentences. (For example, two clauses linked together by an ”and.”) 模型图如下所示： 由图中可以看出来，前面部分都跟普通 self-attention 一样，只是最后通过 $W_{s2}$ 将 attention 权重映射成了一个$ (r * n)$ 的二维矩阵。其中 attention 权重矩阵 $A$ 和句向量的表示 $M$ 的计算公式分别为：$A = softmax(W_{s2}tanh(W_{s1}H^T))$，$M = AH$。 但是需要考虑的是，如果只是这么简单地映射到多维子空间去的话，很容易在不同维度的子空间中产生相同的表达，所以作者提出了一个解决办法，就是加上一个惩罚项 $P=||(AA^T-I)||_F^2$。 这个形式挺像 L2 正则项的，但是其中却大有玄机。很显然有 $0\le a_{i,j}=\sum_{k=1}^na_k^ia_k^j\le1$，当极端情况下，即 $a^i$ 和 $a^j$ 没有重叠的时候，$a_{i,j}$ 的值会为 0，当 $a^i$ 和 $a^j$ 的权重都偏向同一个词的时候，那他们的乘积 $a_{i,j}$ 就会趋近于 1。所以通过这个惩罚项，从对角线看，我们可以使得每个子空间上的 attention 权重集中在单个词上 (乘积趋近于 1)；从对角线之外的项看，我们可以使得不同子空间上的 attention 权重尽量没有重合 (乘积趋近于 0)。这样的话就可以得到一个很完美的二维句向量表示了。 本文将这种 sentence embedding 的方法用在了 NLI 任务上来检验效果，模型图如下所示： 其实就是对 $p$ 和 $h$ 分别求一个二维的句向量表示 $M_p$ 和 $M_h$，然后再分别丢到全连接层得到 $F_p$ 和 $F_h$，接着对 $F_p$ 和 $F_h$ 做一下逐元素乘得到 $F_r$，最后再过一下 MLP 和 softmax 得到输出结果。 Other neural network modelsmLSTM &amp; word-by-word attentionmLSTM (match-LSTM) 在阅读理解上算是一个比较出名的模型了，与上面模型的做法不同的是，这篇文章是直接对 LSTM 单元动手脚。老实说这两个模型我就只跑过 mLSTM，并且这个模型跑起来的速度实在很慢，所以实际上我是直接放弃它了的节奏。 mLSTM 这篇文章是在 word-by-word attention 基础上进行的扩展，word-by-word attention 是出自发表在 ICLR2016 的 Reasoning about Entailment with Neural Attention 这篇文章里的，文章的做法是将 $p$ 和 $h$ 拼接起来，即用处理 $p$ 的 LSTM 状态来初始化 $h$ 的 LSTM 状态。文中还提出了个 word-by-word attention，在处理 $h$ 的每个词的时候，都引入 $p$ 的信息来计算，这种做法可以更好地发现 $p$ 和 $h$ 中词与词之间的关系。 而 mLSTM 这篇文章的作者认为 word-by-word attention 有两个不足： 仅使用了 $p$ 的单一向量来跟 $h$ 做匹配 没有对匹配和冲突这两种情况做区分 针对不足，mLSTM 对原模型进行了改进，他们直接把 attention 权重拼接到了 LSTM 的隐藏状态中去了，公式实在太多不想打了，直接截图附上好了。 注意这 $h_j^s, h_k^t$分别表示 $p, h$ 的隐藏状态。 文中在实验细节部分还说了一下他们用的一个 trick，即他们在 $p$ 中引入了一个特别的词 NULL (用向量 $h_0^s$ 表示)，在做匹配计算的时候，如果 $h$ 找不到跟 $p$ 相匹配的词的话，就会对齐到这个 NULL 上来，这样就增加了悬空对齐的方式，完善了对齐模型。 Decomposable Attention终于轮到了在 NLI 领域大名鼎鼎的 Decomposable Attention 了，这也是我非常喜欢的一篇文章。这篇文章全名是 A Decomposable Attention Model for Natural Language Inference，是 Google 那帮大佬发表在 EMNLP2016 的一篇文章。 文章开头先给出了个例子： Bob is in his room, but because of the thunder and lightning outside, he cannot sleep. Bob is awake. It is sunny outside. 我们以第一个句子作为前提，第二、三个句子作为假设。我们发现第一个句子有个 cannot sleep，第二个句子有个 awake，他们是一组同义词，很好，那么一二句的关系就可能是蕴含关系了。再看第一个句子中的 thunder and lightning 和第三个句子中的 sunny，很明显这两个词是不相符的啊，所以一三句的关系就很可能是矛盾的了。 出于这种思想，Google 这帮大佬们认为，其实我们直接把原问题分解开，看成一个个单词间的对齐问题不就完事儿了？大道至简。 模型分为三步： 对 $p$ 和 $h$ 分别过一下一个共用两层的带 relu 激活函数的前馈神经网络 F，并各自计算 attention 权重 (其中 $\bar a，\bar b$ 为单词的词向量表示) $e_{ij}=F’(\bar a_i, \bar b_j)=F(\bar a_i)^T F(\bar b_j)$ $\beta_i=\sum_{j=1}^{l_b}\frac{exp(e_{ij})}{\sum_{k=1}^{l_b}exp(e_{ik})}\bar b_j$ $\alpha_j = \sum_{i=1}^{l_a}\frac{exp(e_{ij})}{\sum_{k=1}^{l_a}exp(e_{kj})}\bar a_i$ 将 attention 权重和原本的输入变换拼接起来做对齐操作，G 同样为共用的两层带 relu 激活函数的前馈神经网络 $v_{1,i}=G([\bar a_i, \beta_i])$ $v_{2,j}=G([\bar b_j, \alpha_j])$ 将对齐后的向量求和并拼接起来，放入前馈神经网络 H 中 $v_1 = \sum_{i=1}^{l_a}v_{1,i}$ $v_2 = \sum_{j=1}^{l_b}v_{2,j}$ $\hat y = H([v_1,v_2])$ 最后文章又补充了一种 Intra-Sentence Attention 的做法。在计算 $\bar a$ 的时候加入了 attention，令 $f_{i,j}=F_{intra}(a_i)^TF_{intra}(a_j)$， $a’i=\sum{j=1}^{l_a}\frac{exp(f_{ij}+d_{i-j})}{\sum_{k=1}^{l_a}exp(f_{ik}+d_{i-k})}$，其中 $d$ 为两个词之间的距离。这样得到 $a^{‘}_i$ 后令 $\bar a=[a_i, a^{‘}_i]$ ，$\bar b$ 也做同样的操作，后续的处理过程跟上述一样。 Decomposable Attention 算是走了一种极端，把原问题简化成单词间的对齐问题，连 RNN 单元都没有用上，而是直接拿预训练得到的词向量做操作，这样难免会损失掉一些上下文信息。不过这种精神还是很值得学习的，在大家都在堆模型的时候，它另辟蹊径地走出了这么一条路来，即将原问题分解为单词间的对齐问题，这也为后续的很多研究奠定了基础。 因为很喜欢这篇文章，我也用 TensorFlow 实现了一下 Decomposable Attention 的代码。 ESIMESIM 模型算是 NLI 领域未来几年一个很难绕过的超强 baseline 了，单模型的效果可以达到 88.0% 的 Acc，简直吊打一帮人。 ESIM 模型出自 Qian Chen 等人发表在 ACL2017 上的 Enhanced LSTM for Natural Language Inference。也算是对 Decomposable Attention 的一个改进，然后它还加入了句法信息，组成了 HIM 模型，这种思想也为 NLI 后续的研究提供了一个思路。模型图如下所示： 其中左边红框部分就是 ESIM 模型，整体结合起来则是 KIM 模型。 上一节提到，Decomposable Attention 在输入编码时候是直接用前馈神经网络对预训练的词向量做操作，这样会损失掉一些上下文信息。而 ESIM 则在输入和最后的拼接部分对其做了改进，模型分为三步： [Input Encoding] 使用复用的 biLSTM 单元分别对 $p$ 和 $h$ 进行编码，得到 $\bar a, \bar b$ [Local Inference Modeling] 使用 Decomposable Attention 分别对 $p$ 和 $h$ 做权重计算，得到 attention 权重 $\hat a, \hat b$ [Inference Composition] 分别对 $p$ 和 $h$ 进行拼接 (看形式是不是跟上文的 Tree-based CNN 一个套路？)，最后做一个求最大值和均值的操作再将 $p, h$ 拼接起来，过一下biLSTM、FFN 和 softmax 得到最终结果 $m_a=[\bar a;\hat a;\bar a- \hat a; \bar a\cdot\hat a]，m_b=[\bar b;\hat b;\bar b-\hat b;\bar b\cdot \hat b]$ $v_{a,ave}=\sum_{i=1}^{l_a}\frac{v_{a,i}}{l_a}，v_{a,max}=max_{i=1}^{l_a}v_{a,i}$ $v_{b,ave}=\sum_{j=1}^{l_b}\frac{v_{b,j}}{l_b}，v_{b,max}=max_{j=1}^{l_b}v_{b,j}$ $v=[v_{a,ave};v_{a,max};v_{b,ave};v_{b,max}]$ 这就是 ESIM 的模型结构了。除此之外，文中还考虑了加入一些其他的信息，比如 Parsing。文章直接使用了一个 Tree-LSTM 来提取句法信息，除了 encoder 外其他部分的处理跟 ESIM 类似，所以不赘述了。这两个模型合并起来就凑成了 HIM 模型，最终达到了 88.6% 的 Acc。 ESIM 这个模型实在太过于经典，我在携程的矛盾数据集上也想使用这个模型，所以自己也用 TensorFlow 实现了一遍 ESIM 的代码。 KIM &amp; DMAN这两篇是都是出自 ACL2018 的文章，HIM 模型用到了句法信息，启发了一下后续工作的思路，即添加一些外部信息，这两篇 ACL18 的文章正是沿着此路一直走下去的。 首先是 KIM，出自 Neural Natural Language Inference Models Enhanced with External Knowledge，模型上和 ESIM 大同小异。这篇文章加入的是词汇语义关系的信息，比如同义词、反义词、上位词、下位词、同下位词。通过赋予他们不同的权重，在最后拼接的步骤将他们拼接到末尾，得到 KIM 模型，最终跟 HIM 一样取得了 88.6% 的 Acc。 然后是 DMAN，出自 Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference。这篇文章有点耍杂技的味道，在输入端很丧心病狂地将 Word Embedding, Char Embedding, POS, NER, Exact Match 信息拼接起来使用，随后又先用增强学习做一下 DMP (Discourse Marker Prediction) 任务，然后用迁移学习的思想将得到的模型参数拿过来这边使用，以获取句子表示，最后再过一个交互层将 biLSTM 的输出和句子的表示进行交互。。。 我知道我这么说很多人都受不了了，但它模型的确是这样子的，我看的时候也很崩溃。 老实说我很讨厌这种类型的文章，个人觉得除了刷分外真的没有太大的意义。 DIY看了那么多论文和模型，当然自己也得 DIY 一个才算过瘾了啊。 首先就是输入端的编码，biLSTM 单元确实能够更好地保留句子中单词的上下文信息，所以一开始我就直接很常规地使用复用的 biLSTM 分别对 $p$ 和 $h$ 进行编码。 考虑到 structured self attention 确实能够更好地对句子进行表示，所以我直接将 biLSTM 的输出丢入到 structured self attention 中去，得到 $p$ 和 $h$ 的二维矩阵。虽然原文说的是得到句向量的二维表示，但是由于其惩罚函数的缘故，不同子空间的 attention 权重会更加集中于不同的单词上，所以我们可以把它理解为经过了一层 self attention 变换后， $p$ 和 $h$ 保留下来了它们最重要的几个单词的信息。 接下来就到了 decomposable attention 的 showtime 了，将问题再化为 $p$ 和 $h$ 的几个关键单词的对齐问题。最终得到它们对齐后的 attention 权重向量。 最后就是拼接的模块了，既然前人们经过了无数实验试出来了上述的那套拼接方法，那我就拿来直接套用了。具体步骤可以参考 ESIM，不过我把它最后的那步 biLSTM 去掉了，因为我认为对于对齐后的信息再来做 biLSTM 并没有多少意义。 随手用 keynote 画了个简单的模型图如下 (先凑合着吧，虽然确实难看了点，如果哪天要投稿了再来改好了) DIY 模型的代码也放在了 GitHub 上面了，然而现在由于并没有机器，只能等其他模型训练完了后再来试试我这个模型在标准数据集上的结果如何了。希望能够跑个不错的结果出来，借机水一篇文章。 _(:з」∠)_ 模型落地说了这么多，事实上项目中实际上上述所提及的模型都没有用到 (笑)，而是使用了一个我 DIY 模型的简化版本。 因为这个项目其实只需要判断一下两个条款是否矛盾就可以了，而不用考虑其蕴含关系。而对于矛盾的判断并不像蕴含关系那样，我个人认为只要关注前提与假设的个别单词间的关系就足够了，因而像 ESIM 后面的那个 biLSTM 在这个项目中的意义实在不大。事实上在携程的真实数据集中，DIY 模型的简化版本确实在精度和速度上都优于 ESIM。 然而这些数据还是有一些问题的，条款中会出现诸如以下的这类矛盾： A：8 岁以上儿童必须占床；B：2-10 岁儿童可以自行选择是否占床。 A：请于起飞前 120 分钟到达机场；B：请于起飞前 150 分钟到达机场。 像 1. 2. 这类包含数字的矛盾，想让模型判断对真的很难，所以之前携程一直都是使用规则来进行矛盾条款判断。然而规则做法随着商品越来越多，情况越来越复杂，为了保证精度，只能让规则越来越复杂，越来越“过拟合”。可等新的商品一上线又容易引出一堆问题，又陷入重新修改规则的循环中。 本着自己花那么多时间搞出来的东西不上线就亏了的心态，我提议让 leader 把规则给设得严格一点，把准确率给做上去，不管召回率如何。通过规则先去判断出那些模型很难分析出来的矛盾问题，然后再把被规则判定为不矛盾的条款丢给模型进行判断，这样的话就可以在保证精度的前提下，把我的模型给上上去啦，而且也不用三天两头地改规则了。 这是我写的第二篇总结性质的文章，也是我对于这将近一个月的实习经历的总结。之后这个项目应该算是告一段落了吧，因为 leader 说接下来准备让我做情感分析的任务去了。 参考文献 The Stanford Natural Language Processing Group A large annotated corpus for learning natural language inference Natural Language Inference by Tree-Based Convolution and Heuristic Matching Supervised Learning of Universal Sentence Representations from Natural Language Inference Data A Structured Self-Attentive Sentence Embedding Reasoning about Entailment with Neural Attention Learning Natural Language Inference with LSTM A Decomposable Attention Model for Natural Language Inference Enhanced LSTM for Natural Language Inference Neural Natural Language Inference Models Enhanced with External Knowledge Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference]]></content>
      <categories>
        <category>Project</category>
      </categories>
      <tags>
        <tag>Review</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>NLI</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A review on embedding]]></title>
    <url>%2F2018%2F07%2F09%2FA-review-on-embedding%2F</url>
    <content type="text"><![CDATA[这是我第一次写回顾总结性质的文章，真正坐下来开写的时候，才意识到想把一件事物按其历史发展总结展望一下的难度还是蛮大的。 因为明天得做一个组会的 presentation，就想着把我所了解的 embedding 方法给做一个整体性的介绍，希望能够对后续继续入生物序列词嵌入坑的师弟师妹们有所帮助。 文章结构就分成 5 大块，分别为单词级别、句子 / 文档级别、子词级别、字符级别的 embedding 方法，以及最后的总结和展望。 Word-level Embedding单词级别的 embedding 方法之前就有几篇文章写过了，CS224n Lecture 2，CS224n Lecture 3，CS224n Research Highlight 3。 在此再简单提一下单词级别的 embedding 方法的发展。 传统方式 基于分类：最初 NLP 领域是靠一个大词典 (例如WordNet) ，所使用的是上位词和同义词集的信息将单词归到不同的类别中去，以此来表示单词的意思。但是这种方法忽视了单词的语境，并且很难维护。 离散编码：NLP 领域还用 One-hot 编码方式来表示词的意思，但是这种方法的缺点也是显而易见的，即无法度量单词之间的相似度、数据稀疏、维度灾难。 稠密词向量然后 NLP 专家们想出了用稠密的向量来表示一个词的意思，这里需要特别提一下的是一句话 You shall know a word by the company it keeps. 换句话说就是一个词的意思可以由其上下文来表示，这种观点是后续词嵌入模型的根基。 最初模型走的路子大概可以分为两条：直接使用局部上下文信息的方法、基于共现矩阵的方法。 前者的代表为 Word2Vec 算法，后者的代表为 GloVe 算法，具体的内容可以参照以上三篇。 动态方法今年在 NAACL18 上 ELMo 横空出世了，ELMo 词向量的使用把各种 NLP 任务的 state-of-the-art 刷新了一下。 优势ELMo 的优势如下： 能够学习到词汇用法的复杂性，比如语法、语义。 能够学习不同上下文情况下的词汇多义性。 与上述的几种方法不同的是，ELMo 所学得的词向量是动态的，在不同的上下文环境中将会得到不同的词向量表达。 定义ELMo 的思路也是利用单词的上下文信息来表示中心词，与 Word2Vec 等方法简单的线性模型不同的是，ELMo 所用的为 biLSTM 模型，公式如下： 进而最大化其似然函数可得目标函数为： 模型细节ELMo 是双向语言模型 biLM 的多层表示的组合，对于某一个词语 $t_k$，一个 L 层的双向语言模型 biLM 能够由 2L+1 个向量表示。 ELMo 将多层的 biLM 的输出 $R$ 整合成一个向量，$ELMo_k = E(R_k; \theta_e)$。不同层的隐藏状态保留了不同层次的单词信息，一种比较简单的方法是直接拿最顶层的隐藏状态作为词向量，而最好的方法则是将 biLM 层所有层的输出加上一个正则化的 softmax 得到的权重向量。 其中 $\gamma$ 是缩放因子，作用类似于 LN。 论文里头没有给出模型图，因为时间关系，我就随便上网扒了一张 ELMo 的模型图，不过我发现这图是有错误的，姑且放上来凑个数。论文中对模型的描述为 The ﬁnal model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the ﬁrst to second layer. 这里需要说明的是，ELMo 的输入为 char-n-gram embedding，来自 CNN + highway network。 The context insensitive type representation uses 2048 character n-gram convolutional ﬁlters followed by two highway layers and a linear projection down to a 512 representation. 作者发现 ELMo 模型如果能够进行适当的 dropout 或者加入 L2 范式的话，可以使得其最终权重保持在各层 biLM 层的权重均值附近。 用法ELMo 的使用方法也是比较有意思的，有以下两种： 直接将 ELMo 词向量与普通词向量拼接。 直接将 ELMo 词向量与隐藏变量拼接。 补充一下：讲完组会后当天下午看到机器之心推送 NLP领域的ImageNet时代到来：词嵌入「已死」，语言模型当立。标题一看就想搞个大新闻，虽然是一股浓浓的传销性质的标题党文章，但不得不承认 ELMo 这类模型思想和用法确实算是开辟了词嵌入的另一条路，NLP 的迁移学习时代可能真的不远了。 Sentence / Document-level Embedding在句子和文档层面上，由于句子和文档与单词不同，出现的次数很少，并没有像单词一样预训练出词向量以供使用的必要，而是在特定的 NLP 任务中动态生成。 句子和文档层面的 embedding 方法主要分为两类：无监督方法和有监督方法。 无监督方法我在之前的 CS224n Research Highligh 1 介绍了一种简单有效的无监督方法，这里再介绍一下另一篇文章 Distributed Representations of Sentences and Documents。 这篇文章还是 Mikolov 老爷子的作品，所以模型框架跟 Word2Vec 以及 fastText 很相似，所以同样就大概讲一下模型思想。这篇文章分别介绍了句向量和段落向量的表征方式： 句向量对于句向量的训练，本文的做法只是在 Word2Vec 的基础上做延伸工作。本文将得到单词的词向量做简单的求均值或者拼接获得句向量，并且直接根据实际任务来做个分类，在训练任务分类器的过程中同样取得了句向量的表示。这种思想已经有点 fastText 的雏形了，模型的架构图如下： 段落向量关于段落向量的训练，本文提出了两种方法，这里主要介绍一下第一种，即 PV-DM。PV-DM 也是一样的套路，唯一的区别是输入端加入了一个表示段落 id 的 token。该算法主要分为两个阶段： 在训练阶段，我们先学得了模型的参数。 而在预测阶段，随机初始化一个新的段落向量，即目标段落向量。然后将步骤 1 中学得的模型参数给固定住，以同样的方式来训练得出新的段落向量。 作者的观点是借由这个段落向量，我们可以更好地保留住普通词向量所不能包含的特定语境下面的上下文信息，相当于我们多保留了一个该语境下的额外信息。PV-DM 的模型架构图如下所示： 至于 PV-DBOW 架构，跟 Word2Vec 里头的 Skip-gram 模型很相近，思想是拿段落 id 来对相应的上下文信息进行拟合，最终同样可以得到段落向量。但个人认为这种方法很不靠谱，纯粹地拿段落的 id 当作输入来拟合上下文的单词，损失掉了包括词序在内的很多信息。PV-DBOW 的模型架构图如下所示： 因时间关系来不及看另外两篇比较出名的文章，这里留一个坑 Skip-thought vectors，Quick-thoughts vectors，日后了解了这两种模型思想后再来填土。 有监督方法以往的有监督方法只是通过简单的 RNN、CNN 架构来实现，效果往往比无监督方式差，但是最近提出的 InferSent 则取得了非常好的效果，这篇文章是用来做自然语言推导 (NLI) 的，因为之后打算写一篇 NLI 方向的文章，所以这个模型打算放到那边再细讲。(后续补充：InferSent 的模型介绍 ) InferSent 的模型结构很简单，如下图所示。其编码器由 BiLSTM + max pooling 构成。 Subword-level Embedding最早提出 Subword 这个 embedding 思路的应该是 FAIR 的大佬们的 Enriching Word Vectors with Subword Information 这篇文章。 Subword 是 Word 和 Char 之间的一个中间层，考虑的是从形态学的角度来对词的含义进行表征。这里考虑到了几个 word embedding 方法的不足，比如： 在训练词向量的时候，如果某个 word 出现的次数比较少的话，那么它的更新次数也会较少，这样就很难学到这个 word 的高质量的向量表示。 有些词过于稀有，没有在预训练词向量的语料中出现，这样就会导致预测结果无法得到这个词。这里以人名举个例子，假设我们在做阅读理解，”His name is Mikolov, …, __ is a NLP expert. “ 我们一整段话都说对 Mikolov 大神的描述，最后留个空格要填入 Mikolov，然而由于这个姓氏太过于稀有，没有在训练语料中出现，所以模型无论如何也得不到正确答案。 单词的构成可能会包含一些前缀和后缀的信息 (e.g. pre-, sub-, -un, -er, -est)。单词的构成也可能是由不同的成分组合而成的 (e.g. bio + informatics)。这些信息在一定程度上也能够表示单词的含义，然而在 word-level 的 embedding 中，这些信息往往会被忽略掉了。 而 Subword embedding 的提出则能很好地解决上述的问题，在此介绍一下两种比较常见的 subword-level embedding 的方法： ##N-gram 顾名思义，就是以一个固定长度的滑动窗口去对单词的子词进行截取[e.g. apple -&gt; (ap, app, ppl, ple, le)]，最后将各个 subword 的向量求和即可得到整个 word 的向量表示。上述提及的 Enriching Word Vectors with Subword Information 用的就是 n-gram 的方法，在训练时候就是用中心词的 n-gram embedding 来预测目标词。 ##BPE BPE 算法其实是 94 年 Gage 等人提出的，但是 Neural Machine Translation of Rare Words with Subword Unitz 这篇 nmt 文章将其用到了 subword-level 上来。我们可以知道 n-gram 方法虽然能够解决上述的 word-level embedding 的问题，但是它由于是滑动窗口采样的，会导致存在大量冗余信息，也会导致词表大小增大导致算法运行效率变慢。那么如果我们可以对常用词采用 word-level 向量的表示，稀有词再用 subword-level 向量的表示，则可以很好地解决上述问题，因此作者提出用 subword-level 的 BPE 算法来解决这个问题。 BPE 算法的思想其实就是，首先将各个单字符初始化为 token，再统计一下两两相邻 token 的出现次数，将次数最大的 token pair 给合并起来成为新的 token，放回继续统计和合并，最终得到非重叠的 subword。 经过这种组合方式后，常见词最终会由 char 回归到 word 级别，而稀有词则会在 subword 层面上就停止了合并，也就达到了我们的目的。比如 unoffical 就是一个稀有词，而 un 和 offical 则会在语料中大量出现，因而通过 BPE 这种方法，我们最终可以将 unoffical 拆成 un + offical 的组合，进而得到高质量的词向量表示。 Char-level Embedding前面介绍了那么多 -level 的 embedding 方法，最终人们发现，其实最初的 char-level 的词向量也足以很好地完成很多任务了。除此之外，char-level embedding 还可以当做 word-level embedding 的补充来配合使用。 常见的 char-level embedding 只是由 CNN / RNN / RNN-LM 构成。而 Character-Aware Neural Language Models 则提出了 CNN + Highway 的框架，能够很好地捕获 char-level 的信息。因为之前组会上 张开明 刚讲过这个模型在推荐系统上面的应用，所以就不继续讲这个模型了，模型图如下所示： 展望也算把各个层面的 embedding 方法总结了一下了，其实我还看了几年前的几篇关于 embedding 的文章，分别是李嫣然女神的 “后 Word Embedding ”的热点会在哪里？ 和 Adapations and Variations of Word2vec，以及 licstar 的 《How to Generate a Good Word Embedding?》导读 和 Deep Learning in NLP （一）词向量和语言模型。 现在在 18 年回头看 “后 Word Embedding ”的热点会在哪里？ 这篇文章，会发现当初李嫣然提及的四个方向都有所验证： Interpretable relations : 虽然可解释性上目前做得还不够好，但是从形态学方面去入手的研究以及有很多了，比如上文提及的 subword-level, character-level 的研究。 Lexical resources : 虽然训练的词向量可以表示词汇的关系了，但是如果能够再将人工标注的词汇资源加入进去，对于词向量的质量是不是有所提升呢？ Beyond words : 这个就更好理解了，将 embedding 的范围扩大到 sentence 、 paragraph 甚至 document 的层面上去。 Beyond English : 英语作为一种通用语言，所能够获取的训练语料真的十分丰富，而接下来的研究就该转向到那些其他语种的 embedding 中去了。 诚然，时至今日，虽然这一两年还是有 EMLo, InferSent 这些方法的出现，但是 embedding 已经算是一个过气的研究方向了。不过在于生物信息学的方向，对于生物序列 (DNA / RNA / Protein) 的 embedding 工作才刚刚展开，这里涉及到了语料、序列长度、序列切割、无监督分词等一系列的难题，但有理由相信，如果能够加入一些额外的生物信息，在有监督的环境下，还是可能获得一个高质量的序列向量的，届时可能会把目前学界对于生物序列的研究给完全革新一遍，这可能是一项很了不起的贡献。 [虽然我做了那么久的无监督序列分词，至今也没有取得什么好结果，但也只能继续安利师弟师妹们继续踩进这个坑里了，毕竟改变世界的梦想还是要有的。:-) 另外由于本人水平有限，这也是我第一次做如此大规模的总结，如果有写错的地方，希望各位大佬们不吝指教。] 参考文献 深度 | 当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习 ELMo ELMo 最好用的词向量《Deep Contextualized Word Representations》 PV-DM &amp; PV-DBOW Skip-thought vectors Quick-thoughts vectors InferSent Enriching Word Vectors with Subword Information Neural Machine Translation of Rare Words with Subword Unitz Highway “后 Word Embedding ”的热点会在哪里？ Adapations and Variations of Word2vec 《How to Generate a Good Word Embedding?》导读 Deep Learning in NLP （一）词向量和语言模型]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>Review</tag>
        <tag>Deep Learning</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 7 Introduction to TensorFlow]]></title>
    <url>%2F2018%2F06%2F11%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-7-Introduction-to-TensorFlow%2F</url>
    <content type="text"><![CDATA[比较惊讶的是这样的一门课居然会拿出一节课的时间来介绍 TensorFlow，看来老外们也深谙磨刀不误砍柴工的哲理啊。 最近事情实在太多了，这个系列只能等待这阶段忙完后再继续写了。]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 6 Dependency Parsing]]></title>
    <url>%2F2018%2F06%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-6-Dependency-Parsing%2F</url>
    <content type="text"><![CDATA[Parsing 这块之前是我的知识盲区，在上完这节课后觉得还是一知半解，准备把这个坑先留着，等之后翻一些 Parsing 的论文后再同这节课一块写。]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
        <tag>Parsing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Research Highlight3 Bag of Tricks for Efficient Text Classification]]></title>
    <url>%2F2018%2F05%2F09%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20Research%20Highlight3%20Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification%2F</url>
    <content type="text"><![CDATA[本文是 FAIR 在 ACL17 上面的一篇文章。目的是为了解决 NLP 中常见的文本分类问题，并且能够产生词向量作为副产物。 本文提出了大名鼎鼎的 fastText 工具，训练速度很快，而且效果可以跟深度神经网络相当。 本文的作者是 Mikolov，即也是 Word2Vec 的作者，这篇文章在模型架构上跟 Word2Vec 很相似，因为之前已经有详细写过 Word2Vec ，在此就简单说一下他们的区别。 模型架构本篇文章的方法部分大概可以分为三块 (虽然按标题看只有两段)：fastText 模型架构、Hierarchical softmax、N-gram features。 fastText 模型架构 fastText 的模型架构部分跟 CBOW 很相似，这里直接说区别： 区别 CBOW fastText 输入 中心词的上下文 多个单词及 n-gram 特征 编码方式 One-hot Embedding 输出 中心词 文档类别 fastText 的核心思想：将整篇文档的词及 n-gram 向量叠加平均得到文档向量，然后用 hierarchical softmax 做分类。 Hierarchical softmax其实这块没什么好说的，只是把 CBOW 进行的输出端中心词分类换成了对于文档的分类，依照文档的类别建 huffman 树。 N-gram features这块其实是一种很常见的方法，我们知道预训练所得的词向量是固定的，这在于文本分类的任务中，将会丢失一些句子 \ 文档环境中的上下文信息。于是思路就很自然地想到了用 N-gram 的方法来捕获一些上下文信息。 这里需要提及的是本文中使用的 n-gram 是词级别的，举个例子： 关于句子 “Bag of Tricks for Efficient Text Classification”，假设 n 取 3，则整个句子就变为 (“Bag of”, “Bag of Tricks”, “of Tricks for”, “Tricks for Efficient”, …)。 结果 直接上图，效果不错。fastText 宣扬了一下奥卡姆剃刀的思想，“杀鸡焉用牛刀”。不过得注意一下，fastText 的输入是 embedding 后的词向量，本身就包含了单词的相似度、语义、语法信息，这是其除了 N-gram 外提升精度的另一个原因。 本文其实并没有太多好讲的地方，不过跟其同阶段提出的 Enriching Word Vectors with Subword Information 很值得学习，这篇文章我在 A review on word embedding 进行了介绍。 参考文献 Bag of Tricks for Efficient Text Classification CS224n研究热点3 高效文本分类的锦囊妙计 专栏 | fastText原理及实践]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 5 Backpropagation and Project Advice]]></title>
    <url>%2F2018%2F05%2F09%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-5-Backpropagation-and-Project-Advice%2F</url>
    <content type="text"><![CDATA[印象中几乎所有 ML / DL 课程都会介绍一遍 BP 算法，本门课也绕不过 BP 这座山。这节课主要介绍的是 BP 算法和对课程项目的一些建议，因为前者已经是老生常谈的了，所以只能写写后者了。 本门课程是从 4 个不同的角度来讲解 BP 算法的： 通用公式关于 BP 算法的通用公式推导，我还是继续安利一下这篇文章。 电路图课程中举了这么一个例子，我们可以将函数 $f(x,y,z) = (x+y)z$ 视为下图的加法器和乘法器电路： 为了方便计算，我们把中间过程给表示出来，令 $q = x+y , f = qz$，所以则有 $\frac{\partial q}{\partial x} = 1，\frac{\partial q}{\partial y}=1，\frac{\partial f}{\partial q} = z，\frac{\partial f}{\partial z} = q$。从而问题变为求 $\frac{\partial f}{\partial x}，\frac{\partial f}{\partial y}，\frac{\partial f}{\partial z}$，如下图所示： 接着我们从输出到输入一步步地分别反向求导计算，过程如下面图片所示： 这一过程就是我们所说的反向传播的过程了。在反向传播的过程中，我们保留了一个个中间变量，即局部的梯度，这样我们继续反向求导的时候只需用链式法则直接乘以本级的导数即可，节省了大量的重复计算。 再放一个复杂点的例子，请自行跟着图片过一遍流程： 我们注意到电路图的右边其实是一个 sigmoid 门，由于 sigmoid 函数的导数形式很优雅：$\frac{d \sigma(x)}{dx}=\frac{\exp^{-x}}{(1+e^{-x})^2}=(\frac{1+e^{-x}-1}{1+e^{-x}})(\frac{1}{1+e^{-x}})=(1-\sigma(x))\sigma(x)$，因而可以简便地直接将后面几步合并起来带入计算。 流程图流程图的解释方式也是很有意思的，我们可以将网络视为一个有向无环图来理解。 1条路径： 2条路径： 多条路径： 复杂的流程图： 哪怕你带有激活函数，也是以一样的方式来进行反向地传递梯度，我们所需要做的就是先找出某个结点的各个父结点，然后再用链式法则并且求和： 由这些图可以看出，其实对复杂网络进行反向传播计算还是挺麻烦的，所幸现在有一些软件包可以替我们完成这些计算。 误差信号误差信号的传播这块举的是一个两层的神经网络为例： 假设我们想要得到最初输入的导数，并且 $f=\sigma$ 则误差传播的流程如下所示，首先我们求的是最后一层对 $z^{(3)}$ 的误差 $\delta^{(3)}$： 结合以下两图可以得知，对于线性层，我们直接做矩阵乘法即可，而对于非线性层，则是做逐元素相乘。我们可以知道 $W^{(2)}$ 的导数为 $\delta^{(3)}a^{(2)T}$。 最终我们将误差信号从输出层传回了输入层，对于 $W^{(1)}$ 的导数为 $\delta^{(2)}a^{(1)T}$。 在经历 4 轮的轰炸之后，最终 Socher 小哥庆祝了一下大家还存活着。跟着课程过一遍 4 种角度的 BP 算法解读后，应该大家都能够对这个算法有所了解。 这里真的很想吐槽一下，某 A 厂的面试官，对面抛出了一个“你能解释一下 BP 神经网络和深度神经网络的区别吗？”的高难问题，我真的是一副被你打败了的表情，虽然在电话那头的你并不能看见。(手动微笑) #课程项目的建议 这是对于这门课的 project 的建议，不过我认为还是蛮有道理的，姑且把几条建议给放出来： 明确你的任务 明确你的数据集 可以直接使用标准数据集 自己去搜集数据，但要注意别在抓取数据上花费太多时间，这样就本末倒置了 明确你的评估指标 划分数据集 建立起一个 baseline 先去实现一个最简单的模型，比如 LR。空中楼阁是不靠谱的，我们还是要脚踏实地一步一步地走 在训练集和开发集上计算评估指标 分析错误，这个是很重要的一点，先去了解 baseline 的不足之处，后续才能更好地改进模型 如果效果已经很好的话，那证明这个问题太简单了，没有挑战性，就可以省下一堆时间转做其他课题了 复现一下已有的网络模型 同样在训练集合开发集上计算评估指标 分析输出和错误 讲已有的模型作为本课程需要实现的最低标准 [个人补充：将已有的模型扩展到新的领域上也是一种贡献，其实没必要整天就想着整个新模型出来，搞个大新闻] 确保一直围绕在数据左右 可视化数据集 搜集总结、统计 观察错误 分析不同参数下对性能的影响 [个人补充：其实这里最重要的一点就是要确保自己的模型是由数据驱动的，一定要形成”数据驱动”的观念] 尝试修改模型，这时候可以考虑不同模型的优缺点，自行整合它们来达到目的 参考文献 cs224n lecture5 cs224n lecture5 slide BP算法原理和详细推导流程 CS224n笔记5 反向传播与项目指导]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 4 Word Window Classification and Neural Network]]></title>
    <url>%2F2018%2F05%2F08%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-4-Word-Window-Classification-and-Neural-Network%2F</url>
    <content type="text"><![CDATA[这节课 Socher 小哥讲的是根据文本的上下文来进行分类预测的问题。 分类任务分类问题是很常见的机器学习任务了，在 NLP 领域，比较常见的分类例子有常见的情感、命名实体、决策等机器学习也常做的分类问题，也有通过上下文来预测其他单词的 (e.g. 阅读理解的填词)，还有直接预测句子的 (e.g. 翻译)。 直观的分类传统机器学习中经常使用的是 LR, SVM 之类的方法来找到分类的决策边界。 Softmax这又是个大家常见的老朋友了，softmax 函数为 $p(y_j = 1|x) = \frac{\exp(W_jx)}{\sum_{c=1}^C \exp(W_cx)}$。 训练时候当然是为了让其概率 $p$ 最大了，所以为了方便计算，我们可以对概率取个负的对数作为其目标函数，其实这个损失函数就等同于交叉熵了。 $\begin{eqnarray} H(\hat y,y)&amp;=&amp; -\sum_{j=1}^{|V|}y_j\log(\hat y_j) \ &amp;=&amp; -\sum_{j=1}^{C}y_j\log(p(y_j=1|x)) \ &amp;=&amp;-\sum_{j=1}^Cy_j\log(\frac{\exp(W_jx)}{\sum_{c=1}^C \exp(W_cx)}) \end{eqnarray}$ 即 $H(p, q) = -\sum_{c=1}^C p(c)\log q(c)$，交叉熵也可以被重新写成 KL 散度的形式：$H(p,q) = H(p) + D_{KL}(p||q)$。因为 $H(p)$ 在这里是为 0 的 (即使不为 0，那它也是一个固定值，对于优化并没有什么帮助)，所以要最小化这个等式就等同于最小化 KL 散度。 这里要注意的一点是 KL 散度并非是指距离，而是一种非对称的衡量 $p$ 和 $q$ 之间概率分布差异的标准，其具体公式为 $D_{KL}(p||q) = \sum_{c=1}^C p(c)\log\frac{p(c)}{q(c)}$。 我们回到之前的 $H(p, q)$ ，因为这里的 $p$ 是 one-hot 的，所以实际上剩下的就只有正类的负对数项了，所以在整个数据集 ${x_i,y_i}^N_{i=1}$ 的目标函数为：$J(\theta) = \frac{1}{N}\sum_{i=1}^N-\log(\frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c)}})$。 再加上正则项即可得最终的目标函数为：$J(\theta) = \frac{1}{N}\sum_{i=1}^N-\log(\frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c)}}) + \lambda\sum_k \theta_k^2$。 分类任务中词向量的更新在传统的机器学习问题里，模型的参数是由那些权值矩阵 $W$ 的列组成的，一般都不会太大，我们只需要更新它的决策边界： 而在深度学习里，情况就截然不同了，我们除了更新决策边界之外，还可以更新我们的词向量 $x$： 要知道参数一多，模型就容易发生过拟合，课程中举了一个例子。在预训练的词向量里，”TV”, “telly” 和 “television” 这三个词是一块的，但由于情感分类的语料中的训练集只包含 “TV” 和 “telly”，导致 re-training 后这两个词跑到其他地方去了，不再跟其同义词 “television” 一起。 那么什么时候应该 re-train 词向量，什么时候又该使之固定不动呢？Socher 小哥给出了一点人生经验：如果训练集较小的时候，就不要再训练词向量，因为这样会破坏其原有结构；如果我们有一个很大的训练集的话，那我们在训练模型中也一并再训练词向量将会取得更好的效果。 窗口分类在文本分类的任务中，我们其实是很难对于单个单词来进行分类的，因为语义中会有歧义出现，例如： “to sanction” 的意思可以是 “to permit” 或者 “to punish”；”to seed” 的意思可以是 “to place seeds” 或者 “to remove seeds”。 而在命名实体中也很容易产生歧义，例如：”Paris” 可以指 “Paris, France”，也可以指 “Paris Hilton”，”Hathaway” 可以指 “Berkshire Hathaway”，也可以指 “Anne Hathaway”。 那么该如何去消除这些歧义呢？一个很自然而然的想法就是用上这个单词的上下文信息。 有种做法是将窗口内的词向量来求一个均值 / 加权均值，但是这样会丢失其位置信息。 还有另外一种做法是将其窗口内的词向量给拼接起来，例如我们取窗口长度为 2，则新的向量的维度为 $X_{window} = x \epsilon \mathit{R}^{5d}$。 那么如何对这种拼接起来向量进行更新呢？答案是跟原本一样，我们可以将该词向量再拆分开来算，只是我们需要更加小心别算错了。这里需要知晓的是，在更新过程中，计算代价比较高的有两部分：矩阵乘法 $f = Wx$ 和 $\exp$。当然，矩阵乘法的速度还是比循环快很多。 这种拼接的方法是很有好处的，比如之前的 “Paris” 的例子，假如我们在其窗口内发现了 $X_{in}$ 这个词向量，就知道它代表的是一个地名而非人名了，这样歧义就消失了。 softmax (= LR) 的分类效果其实是十分有限的，它只是一种线性的分类器： 而神经网络则可以提供非线性的决策边界： 神经网络这里课上又讲了一些神经网络的基本知识，包括神经元、激活函数、前向传播、反向传播、链式求导之类的老调调，这里就偷懒不写了 。 关于 BP 算法，这里有篇文章我觉得写得不错。 间隔最大化目标函数最后选择性得写一下目标函数，在设计目标函数的过程中，我们用 $S$ 表示正确分类的得分，$S_c$ 表示误分类的得分。其中 $S = U^Tf(Wx+b), S_c = U^Tf(Wx_c + b)$，我们通过负采样来得到负例。 一种很朴素的想法是直接最大化 $(S - S_c)$，也就是说我们只需要要求 $S$ 的值高于 $S_c$ 就可以了，并不要求两个值的实际大小为多少，我们在计算损失的时候只需简单地计算 $(S_c - S) &gt; 0$ 时候的错误。这种限制下的目标函数为：$J = \max(S_c - S, 0)$。 这么做的要求实在太低了，我们可以往这里加入一个缓冲区域，令 $J = max(1 - S + S_c, 0)$。 这种做法可以让我们在训练中更加关注那些难以区分的样本。 参考文献 cs224n lecture4 cs224n lecture4 slide CS224n笔记4 Word Window分类与神经网络]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
        <tag>Ambiguity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Research Highlight2 Linear Algebraic Structure of Word Senses, with Applications to Polysemy]]></title>
    <url>%2F2018%2F05%2F07%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Research-Highlight2-Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy%2F</url>
    <content type="text"><![CDATA[这是一篇最终收录在 TACL2018 上面的文章，讲的是词向量中一词多义的问题。 如何表示一词多义我们知道预训练好的词向量是固定的，对于词表中的每个词它都有一个固定的向量表示，我们如果把这些词向量投影到二维空间里去，就可以发现相关的词的欧几里得距离会较小，即他们会聚集在一块。 这种固定的词向量表示虽然能够表征出单词的意思，但是也带来了另一个问题，即当一个词有多个意思的时候，那它在向量层面到底是怎么表示的？ 课上以这个 “tie” 为例，它可以表示足球比赛中的平局，也能够表示服饰范畴内的领带，还能够表示绳子打结。那么如果在投影 “tie” 这个词的时候，它应该落在上图中的哪个位置呢？ 论文里头给出了答案，它的向量实际上是各个意思的平均值，也就是说词向量可以包含了它各个意思的信息。 如何复原各个意思那么这样又带来了一个问题，既然它已经被求平均了，那么我们可以把它的各个意思都给复原出来吗？ 论文指出单词的意思是由下式来进行稀疏编码的： 其中的上下文向量 $A_i$ 大概包括 2000 个词，而 $\alpha_i$ 则表示特定上下文向量的系数，一般选择 5 个上下文向量来做相加，最后加上一个无关紧要的噪声项。 论文里头说这些参数可以由标准 k-SVD 算法求出，所以不同的词义就可以通过这种稀疏编码的方式复原回来了，复原结果如下： 最后拉上一帮各国的研究生来“人工标注”，即问他们下图左方的一类词是否同某个词相关，再来同复原的结果做对比。下图右方的结果表明这种方法大概可以达到非土著的区分水平，说明效果还是挺不错的。 总结最后 TA 作了个总结： 词向量是可以捕获一词多义的信息的 词向量是由该词的不同意思的向量做线性叠加而得到的 一词多义可以通过稀疏编码的方式复原出来 这种复原方法可以达到 non-native English speaker 的水平。 参考文献 Linear Algebraic Structure of Word Senses, with Applications to Polysemy CS224n研究热点2 词语义项的线性代数结构与词义消歧]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
        <tag>Polysemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 3 Advanced Word Vector Representations]]></title>
    <url>%2F2018%2F05%2F07%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-3-Advanced-Word-Vector-Representations%2F</url>
    <content type="text"><![CDATA[这节课一开始先回顾了一下上节课的内容，Word2Vec。 Word2Vec 总结接着对 Word2Vec 算法作了一个总结： Word2Vec 遍历了整个语料库中的每一个词。 Word2Vec 算法的主要思想为通过一个单词的上下文信息中得到这个单词的意思，即通过中心词来预测其上下文信息 (SG) 和通过上下文信息来预测中心词 (CBOW)。 Word2Vec 捕获的是共现词中是否同时出现的信息。 这里由第 3 点引出了一种新的思路，Word2Vec 算法只是捕捉了两个词是否同时出现的信息，那我们是不是可以直接去捕捉单词间共现次数的信息呢？很显然答案是肯定的，这也就是本节课的内容，GloVe。 基于共现矩阵的词嵌入模型在讲述 GloVe 算法之前，我们先讲一下较为原始的方法。首先我们需要通过大量的语料文本来构建一个共现矩阵 (Co-occurrence Matrix)。矩阵的构建方式有两种：document-based 和 windows based。 前者一般用于主题模型 (LSA)，由于统计的是全文的信息，所以这种矩阵很难描述单词的语法信息。后者类似于 Word2Vec，需要指定一个统计的窗口大小，只在窗口的范围内统计单词的共现次数，这种方法可以同时捕捉到语法信息 (POS) 和语义信息。 举个例子，假设语料由三句话组成： I like deep learning. I like NLP. I enjoy flying. 那么如果我们把其窗口的大小设为1，并使用对称窗口的方法，则其共现矩阵如下： counts I like enjoy deep learning NLP flying . I 0 2 1 0 0 0 0 0 like 2 0 0 1 0 1 0 0 enjoy 1 0 0 0 0 0 1 0 deep 0 1 0 0 1 0 0 0 learning 0 0 0 1 0 0 0 1 NLP 0 1 0 0 0 0 0 1 flying 0 0 1 0 0 0 0 1 . 0 0 0 0 1 1 1 0 如果直接简单的以共现向量当做单词的词向量的话，虽然不同的词向量之间就不再像 One-hot 一样是正交的了，可以一定程度上用来计算单词间的相似度，但是还是存在许多问题： 随着词表的增加，词向量的维度也得跟着增加。 维度灾难问题。语料足够大的时候词表也会很长，导致向量长度过大，训练的代价高昂。 数据稀疏。与 One-hot 类似，共现矩阵中不为 0 的维度数量很少，会使得后续的分类模型鲁棒性下降。 共现矩阵的降维处理为了解决以上问题，语言学家们自然而然地想到了将共现矩阵进行降维，进而得到单词的稠密表示 (dense representation)。 那么如何对共现矩阵进行降维呢？一个很常见的方法就是使用奇异值分解 (SVD)。SVD 的基本思想是，通过将原共现矩阵 X 分解为一个正交矩阵 U，一个对角矩阵 S，和另一个正交矩阵 V 乘积的形式，并提取 U 的 k 个主成分（按 S 里对角元的大小排序）构造低维词向量。具体原理推导可以参考 leftnoteasy的博文。 除此之外，对于共现矩阵 X 的处理也有很多值得一提的 Hacks： 部分的单词 (the, he, has) 出现的次数过多，使得它们对于语法信息的影响过大。改进的方法就是限制高频词的最大频次 (min(X, t), with t~100)，或者干脆直接停用高频词。 带权重地统计窗口。距离中心词跃进的词对于词义的贡献就越大。 使用 Pearson 相关系数来替代掉词频，并把负值置 0。 以上的方法虽然简单粗暴，但是还是取得了很不错的结果。 然而 SVD 算法有几个根本性的问题没法解决： SVD 是一个计算复杂度高昂 (O($mn^2$)) 的算法，无法在实际环境中使用。 SVD 方法不方便处理新词或者新的文档，如果加入一些新的语料后，就需要重新再进行 SVD。 SVD 的画风跟其他的 DL 模型截然不同。 基于共现次数统计的方法 VS 直接进行预测的方法讲完了 SVD 这类基于共现次数统计的方法和之前的 Word2Vec 这类直接进行预测的方法，课程中列出了以下这一张两种方法的对比图，两种方法的优势和劣势总结得十分到位。 那么有没有一种方法能够同时结合以上这两种方法的优势呢？这就轮到本课的主题 GloVe 出马了。 GloVe (Global Vectors)为了结合以上两种方法的优势，GloVe 模型既使用了语料库的全局统计特征 (全局共现次数统计)，也使用了局部的上下文特征 (窗口)。因而 GloVe 模型引入了共现概率矩阵 (Co-occurrence Probabilities Matrix)。 直接上论文中的例子： 该矩阵的第一个元素为 ice 出现时 solid 出现的概率，第二个元素为 ice 出现时 gas 出现的概率，以此类推。由共现概率矩阵的值可以看出，概率比率 ($F(w_i, w_j, \hat w_k) = \frac{p_{i,k}}{p_{j,k}}$) 的取值是有一定规律的。 规律总结如下： $F(w_i, w_j, \hat w_k)$ $w_j, w_k$ 相关 $w_j, w_k$ 不相关 $w_i, w_k$ 相关 趋近 1 很大 $w_i, w_k$ 不相关 很小 趋近 1 也就是说，与原本的概率值相比，概率比例能够更好地表示出两个单词间的相关关系。这是一个很简单但是很有用的规律，如果我们用词向量 $w_i, w_j, w_k$ 通过某种函数计算 $F(w_i, w_j, \hat w_k)$，能够同样得到这样的规律的话，就意味着我们词向量与共现矩阵具有很好的一致性，也就说明我们的词向量中蕴含了共现矩阵中所蕴含的信息。 于是接下来就到了原作者神奇的脑洞时间了。 先上公式：$F(w_i, w_j, \hat w_k) = \frac{p_{i,k}}{p_{j,k}}$ 这里等号的左端为全局统计求得的值，右端的 $w_i, w_j, w_k$ 就是我们要求得的词向量，而函数 F 是未知的，作者确定 F 的过程真是让人瞠目结舌。过程如下所示： $\frac{p_{i,k}}{p_{j,k}}$ 这个值考察了 $w_i, w_j, w_k$ 三个词两两之间的相关关系，但是这样很难进行 F 的求解，所以更好的方法是先去考察 $w_i, w_j$ 两个词之间的关系，线性空间中的相似性关系自然想到的是两个向量的差 $(w_i - w_j)$，所以我们可以把 F 的形式转化为 $F(w_i - w_j, w_k) = \frac{p_{i,k}}{p_{j,k}}$。 $\frac{p_{ik}}{p_{jk}}$ 是一个标量，而 F 是直接作用在 $w_i - w_j$ 和 $w_k$ 这两个向量上的，为了把向量转化为标量，自然地就想到了用内积的方法，所以我们可以把 F 的形式进一步转化为 $F((w_i - w_j)^T w_k) = F(w_i^Tw_k - w_j^Tw_k) = \frac{p_{i,k}}{p_{j,k}}$。 此时 F 的公式的形式为 $F(w_i^Tw_k - w_j^Tw_k) = \frac{p_{i,k}}{p_{j,k}}$。等号左边为差的形式，右边则是商的形式，要把差和商关联起来，作者又想到了用取指数的形式，即 $\exp(w_i^Tw_k - w_j^Tw_k) = \frac{\exp(w_i^Tw_k)}{\exp(w_j^Tw_k)} = \frac{p_{i,k}}{p_{j,k}}$。 现在形式就很明朗了，我们只需让 $\exp(w_i^Tw_k) = p_{i,k}, \exp(w_j^Tw_k) = p_{j,k}$ 等式就能够成立。 那么如何让 $\exp(w_i^Tw_k) = p_{i,k} = \frac{x_{i,k}}{x_i}$ 成立呢？只需让 $w_i^Tw_k = \log\frac{x_{i,k}}{x_i} = \log x_{i,k} - \log x_i$。 又因为作为向量，i 和 k 的顺序交换后 $w_i^Tw_k$ 和 $w_k^Tw_i$ 应该是相等的，即它们应该是对称的。但上式的右边显然不符合这个条件，所以为了解决这个问题，作者又引入了两个偏置项 $b_i, b_k$，这样模型就变成了 $\log x_{i,k} = w_i^Tw_k + b_i + b_k$，其中 $b_i$ 包含了 $\log x_i$。此外还加入了 $b_k$ 来保证模型的对称性。 因此，我们就可以得到 GloVe 的目标函数了：$J = \sum_{i,k}(w_i^Tw_k + b_i + b_k - \log x_{i,k})^2$。 再考虑到出现频率越高的词对权重的影响应该越大这个原则，我们需要在目标函数里加一个频率权重项 $f(x_{i, k})$，所以最终的目标函数为：$J = \sum_{ik}f(x_{i,k})(w_i^Tw_k + b_i + b_k - \log x_{i,k})^2$。 对于这个频率权重项 $f(x_{i, k})$ 我们需要确保其为非减的，并且类似于之前对 (the, he, has) 这类常见词的处理方式，当词频过高的时候，频率的权重项不应该过大，因而需要将 $f$ 控制在一个合适的范围，所以频率权重函数 $f$ 的公式如下： 最终作者经过实验得出当 $x_\max = 100, \alpha = 0.75$ 是一个比较好的选择。 以上就是 GloVe 算法的完整推导流程，这里可以看到，最终我们得到的是 $w_i, w_k$ 两个权重矩阵，它们都捕捉到了单词的共现信息，在实际使用中，实验证明直接将二者简单的相加，得到的权重矩阵就是效果最好的词向量表示，即 $w_{final} = w_i + w_k$。 GloVe 优点 训练速度快。 可以扩展到大规模的语料。 也适用于小规模语料和小向量。 如何评估词向量评估词向量质量高低的方法分为两种： Intrinsic (内部) 和 extrinsic (外部)。 IntrinsicIntrinsic 评估的方式为专门设计单独的试验，由人工标注词语或句子相似度，与模型结果对比。 这种方法看似比较合理，并且计算速度也很快，然而却不一定对实际应用有帮助。有时候你花很多时间来调整词向量，使得内部评估方式的得分看起来很高，结果在实际应用上一跑预测结果却下降了。(尴尬又不失礼貌的微笑.jpg) 对于词向量模型来说，一个常用的 Intrinsic 评估是向量类比 (word vector analogies)，它评估了一组词向量在语义和句法上表现出来的线性关系。 以上例而言，我们给定了一组词 $(a, b, c, d)$ 我们要验证的是 $d$ 与向量 $(x_b - x_a + x_c)$ 的余弦距离的值要最接近于 $d$ 这个词本身。 ExtrinsicIntrinsic 评估的方式为通过对外部实际应用的效果提升来体现。 耗时较长，不能排除是否是新的词向量与旧系统的某种契合度产生，需要至少两个subsystems同时证明。这类评测中，往往会用pre-train的向量在外部任务的语料上retrain。 词向量训练经验直接上结论： GloVe 的效果在一些任务上可能会比 Word2Vec 更好，但有些时候二者并没什么区别，由于 Word2Vec 出现得更早，原理也更简单易懂，因而目前为止使用得还是很广泛的。 语料质量 &gt; 语料数量。记得有某个 kaggle 的关于 Quora 句子相似度的比赛，我们组使用了个不错的模型，结果被其他组用普通的 LSTM 模型吊打得花枝乱颤，归其原因除了预处理没有做好外，我们使用的是泛用的语料训练得到的词向量，而那组人用的是专门的 Quora 语料。 向量维度、窗口大小、窗口是否对称对词向量质量也有影响，具体如图。然而这是人类语言中的效果图，在不同的领域中参数还是需要根据实际情况来调整的。比如我之前做的 RNA 序列问题，最终最好的结果所取的窗口大小大于图中所示，向量维度反而是小于的。 GloVe 相比 Word2Vec 更加稳定。 维基百科的语料好于新闻的语料。 在训练集大小较小的时候，在模型的训练过程对词向量进行 retrain 的话，会导致整个向量空间原有的几何结构被破坏，从而使得泛化能力变差，所以当训练集不够充分的时候不要 retrain 词向量。当训练集足够大的时候，retrain 词向量往往可以使预测精度得到提升。 从菜爸爸那里得知的一个 trick，可以同时使用两个 pretrain 的词向量，一组固定，一组随着模型的训练进行 retrain，可以使得精度提高。 参考文献 cs224n lecture3 cs224n lecture3 slide GloVe: Global Vectors for Word Representation 强大的矩阵奇异值分解(SVD)及其应用 理解 GloVe 模型 (Global vectors for word representation) GloVe模型 (Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估 CS224n笔记3 高级词向量表示]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding]]></title>
    <url>%2F2018%2F05%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20Research%20Highlight1%20A%20simple%20but%20tough-to-beat%20baseline%20for%20sentence%20embedding%2F</url>
    <content type="text"><![CDATA[cs224n 这门课很有意思的一个地方在于教授会让 TA 在中场休息时候花个 5 分钟左右的时间来讲一下当前的研究亮点。我觉得这点很可取，这么做有助于学生开拓思路、紧跟当下热点，可惜这种做法在国内的大学中是很少能够看见。 这个小讲座讲的是发表在 ICLR17 的一篇文章，A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS。 动机本文是用无监督方法做句子级别的 embedding，用的是一个十分简单但却又很有效的传统方法，这在神经网络泛滥的年代算是一股清流了。 作者做 word2sen 的动机是想要得到句子的词向量，这样就可以进行句子间的相似度计算了。除此之外，还能在这些句向量的基础上构建分类器来做情感分析的研究。 已有方法稍微介绍一下目前已有的方法。 基于无监督的线性变换的方法。例如简单的对词向量求平均，或者对词向量进行加权平均 (例如以 TF-IDF 为权值)。 基于有监督的神经网络的方法。例如各种满天乱飞的 CNN, RNN (Recurrent), RNN (Recursive) 模型得到的句向量。 本文方法作者将该算法称为 WR，W 表示 Weighted，根据预设的超参数和词频给每个词向量赋予权重。R 表示 Removal，使用PCA移除句向量中的无关部分。 算法流程分为两步： W 步：对于句子中的每个词向量乘以一个独特的权值，即 $\frac{a}{a+p(w)}$，其中 $a$ 为一个常数 (论文中建议 $a$ 的范围： [$1e−4,1e−3$] )， $p(w)$ 为该词的频率。 R 步：计算语料库所有句向量构成的矩阵的第一个主成分 $u$，让每个句向量减去它在 $u$ 上的投影 (类似 PCA)。其中，一个向量 $v$ 在另一个向量 $u$ 上的投影定义为：$Proj_uv = \frac{uu^Tv}{||u||^2}$。 概率论解释 其原理是，给定上下文向量，一个词的出现概率由两项决定：作为平滑项的词频，以及上下文。其中第二项的意思是，有一个平滑变动的上下文随机地发射单词。 这篇文章我没有看过原文，这里其实并不是完全理解，等之后再来填坑。 结果 取得了一个不错的结果，但是比 LSTM 效果好这种说法不太妥当，这得取决于实际的任务。 不过无论如何，这种方法运行耗时短，结果又不错，可以当做一个很好的 baseline。 参考文献 A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS CS224n研究热点1 一个简单但很难超越的Sentence Embedding基线方法]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 2 Word Vector Representations word2vec]]></title>
    <url>%2F2018%2F05%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-2-Word-Vector-Representations-word2vec%2F</url>
    <content type="text"><![CDATA[之前的第一课里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。 对于这个现象，有一种说法是： 语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。 这里就涉及了深度学习中输入的问题，也是所有 NLP 任务中都无法绕过的问题，即在计算机里如何表示一个词？ 答案显而易见，就是本节课的主题词向量 (Word Vector)。为了方便理解，我补充了一些课程中没有讲到的内容。 如何表示词的意思姑且先看看 Webster 词典中对于 meaning 的定义： the idea that is represented by a word, phrase, etc. the idea that a person wants to express by using words, signs, etc. the idea that is expressed in a word of writing, art, etc. 而在语言学中，对于 meaning最常见的解释为： signifier ⟺ signified (idea or thing) = denotation 在计算机中我们该如何得到可用的词意以上的定义由具体到抽象，在 NLP 领域中，主要是针对第一条 word representation进行研究。 最初 NLP 领域使用的是分类系统来对词的意思进行表示，比如 WordNet。我们可以通过查询单词的上位词以及同义词集来得知这个词的意思。￼ 比如上图，我们查询 panda 的上位词，可以得到 procyonid (浣熊科)、carnivore (肉食动物) 等词。查询 good 的同义词，可以得到 full、 honorable、 beneficial、ripe 等词。 这就是一种离散的表示 (Discrete Representation)，然而这种表示方式存在很多的问题： 词语有着独特的语境。以上面的同义词为例，在查询 good 的时候我们发现， ripe 和 honorable 都为其同义词，但是我们在描述一件事物为 good 的时候，如果使用其同义词 ripe 或者 honorable 来代替的话，很多时候都会显得很别扭。 语言的变化日新月异，每天都有大量的新词涌现出来，而使用这种表示方式的话，很难及时更新旧词的含义和对新词进行定义。维护一个类似 WordNet 这样的数据库是需要花费大量的人力的，代价非常高昂。并且，使用人力去维护词库的话，必不可免地会存在人的主观性的影响。 通过这种方式，我们能够得到的只是词的上下位词的信息，以及其同义词集信息，这种信息都只是一个二分类的结果，导致我们很难对于两个词之间的相似度给出一个准确的度量方式。 在绝大多数的基于规则和基于统计的 NLP 工作中，都将单词视为一种原子符号来进行处理。 比较常见的做法就是用 one-hot 向量来表示一个词，这是一种 localist representation。 从 symbolic representations 到 distributed representationsOne-hot 看着是一种简单粗暴又可行的表示方法，然而在实际使用中，像 one-hot 这种离散的 symbolic representations 是存在一些问题的： One-hot 向量是正交的，这种表示方法是无法得到不同单词间的相似度的。 One-hot 向量存在着数据稀疏的问题。 因其表示的方式的问题，随着所用词表的扩增，One-hot 向量还会出现维度灾难。(Dimensionality: 20K (speech) – 50K (PTB) – 500K (big vocab) – 13M (Google 1T) 因而，我们需要去探讨一种新的词向量的编码方式。 You shall know a word by the company it keeps. – J. R. Firth 1957: 11 这里就提出了一种思路，即我们可以通过一个单词的上下文信息中得到这个单词的意思。 这是现代统计自然语言处理最成功的思想之一，事实上这也是 Word2Vec 算法的根基。 Word2Vec定义首先我们需要先定义一下预测单词上下文的模型及损失函数： $p(context | w_t) = …$ $J = 1 - p(w_{-t} | w_t)$ 这里的 $w_{-t}$ 表示除了 t 之外的上下文。 Word2Vec 的主要思想Word2Vec 有两种计算模型，两种模型的思路是截然相反的： Skip-grams (SG) : 通过中心词来预测其上下文信息 Continuous Bag of Words (CBOW) : 通过上下文信息来预测中心词 先上一张课件中 SG 模型的预测图方便大家理解，这里我们就是通过 banking 这个中心词来对其上下文信息进行预测，学习的目的就是要最大化上下文中各词的条件概率。 这里的上下文的条件概率 $p(w_{-t}|w_t)$ 是由 softmax 得到的： 然后，目标函数定义为所有位置的预测结果的乘积： 为了方便计算，加个负对数得到最终的损失函数，这样问题就转化为最小化损失函数了： 接下来就是上几张大家不知道见过多少遍的经典模型图了，以下分别为 CBOW 模型和 SG 模型： 模型流程这里参考知乎上crystlajj的回答，并偷懒盗用了不少图，以 CBOW 模型的流程为例： 输入层：上下文单词的 onehot. {假设单词向量空间dim为V，上下文单词个数为C} 输出层的 label ：中心词的 onehot。 权重矩阵 W 和 W’：维度分别为 {V*N} 和 {N*V}，并将权重矩阵初始化。要注意最后得到的输入端的 W 即为单词的词向量表示，词向量的维度为N（每一列表示一个词的词向量）。 所有上下文单词的 onehot 分别乘以共享的输入权重矩阵 W。 将每个单词的 onthot 与 W 相乘所得的 {1*N} 维向量相加求平均作为隐层向量。并将得到乘以输出权重矩阵W’，得到一个 {1*V} 维的向量。 将得到的 {1*V} 维向量经过 softmax 函数处理得到 V-dim 概率分布，概率最大的index所指示的单词为预测出的中心词（target word）。 与true label的onehot做比较，损失函数越小越好。 再偷几张图举个例子，让大家跟着走一遍CBOW模型的流程： 以上就是CBOW 模型的流程，这里再附上一张课件中给出的 SG 模型的流程，可以对比一下。 以上就是 Word2Vec 的两种模型的算法流程。其实这种 distributed representation 的思想其实很早就有了，而真正火起来是在 13 年 Mikolov 大神的两篇文章之后。 因为这个方法实际上存在着一些问题。比如训练词向量模型时我们输入端和输出端用到的依旧是 onehot 编码，维度灾难问题仍然存在。如果语料足够大的话，onehot 编码的长度就很大，每轮学习需要更新大量的参数，将会导致训练速度慢到难以接受的地步，如果语料不够充足的话，则很难学到高质量的词向量表达。 Word2Vec 优化因而 Mikolov 大神就提出了两种高效的训练方法： Hierarchical softmax Negative sampling 这部分是课程里头没有涉及的内容，并且公式太多太繁杂，在这里只做一个大概的介绍（实在是懒得写了 doge），具体的推导和源码解析可以参考 falao_beiliu 和 Hankcs 的文章。 Hierarchical softmax以 CBOW 为例，HS 的方法就是在输出层使用一颗 huffman 树来提升训练效率，模型图如下所示： Huffman 树在这里就不科普了，HS 方法中的 huffman 树的每个叶子结点表示一个单词，可以被 01 唯一地编码。每个非叶子结点相当于一个神经元。 由于 huffman 树的特点可以使得编码长度最短，这里我们将单词以词频来建立huffman 树，就可以使得常用词的路径较短，非常用词的路径较长。HS 方法就可以将原本查找目标词的多分类问题转化为多个二分类问题，在每一个非叶子结点进行 softmax 运算，根据结果选择向左下走或者向右下走，进而直到走到叶子结点，就可以得知预测的目标词是什么。通过这种方法，我们每次更新的只是走过的路径上的参数，而非全部的参数，可以使得效率大幅度提高。 SG 模型的 CBOW 方法同理，结构示意图如下所示： Negative Sampling回想一下之前的算法，我们输出的 label 都是一个词的 one-hot 编码，维度为整个词库的大小，这里的目标词的 index 即为分类任务的正例，其他的词都为负例，也就是说在一个单词数为 N 的词库中，我们的分类任务的正例为 1，负例为 N - 1。正常 N 值会远远大于 1，但是每次更新中，所有的负例的参数权重也会跟着一块更新，在 Mikolov 看来这其实是一个很没有效率的事情，我们大可以把负例减少来提高效率并且对于最后的结果不会产生大的影响。 因此就提出了 NS 方法，顾名思义，NS 即随机挑选出一些负例。采样的算法思想都应该保证频次越高的样本越容易被采集到，因此，NS 算法基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语： $len(w) = \frac{cnt(w)^N}{\sum_{u\epsilon D}cnt(u)^N}$ 所以就可以根据词频来分配线段中每个词所占有的长度了。这里 Word2Vec 为了提高效率，用了点小 trick，即将原本线段标上 M 个刻度，这样就不需要浮点数的操作，而是直接使用 0-M 间的整数，经过查表就可以得知该取哪个单词了。 注意这个 len(w) 的计算有包含一个幂计算 (N)，这实际上是一种平滑的策略，可以增大低频词的值，使之更容易被采样到。 以上大概就是本节课的内容和扩展，实际上课程里头还包含了一些数学知识的复习和 sgd 算法的推导，觉得过于老生常谈，就不拿出来说了，之后如果有相似的内容也会一样跳过。 参考文献 cs224n lecture2 cs224n lecture2 slides cs224n笔记2 词的向量表示 word2vec原理推导与代码分析 深度学习word2vec笔记之算法篇]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 1 Introduction to NLP and Deep Learning]]></title>
    <url>%2F2018%2F05%2F03%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-1-Introduction-to-NLP-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[开的一个新坑，想记录一下自己学习 CS224n 的经历，希望可以坚持到最后。 CS224n 是 NLP 领域很出名的一门课程，由 NLP 领域宗师级的人物 Christopher Manning 和 Richard Socher 共同授课。这门课程讲授的是 DL 下的 NLP 知识及应用，并在课间也介绍了目前各个 NLP 领域的前沿亮点研究，这种将理论基础与当下研究亮点相结合的模式我认为很适合让国内高校借鉴。跟大部分的课程一样，第一节课的内容还是比较轻松的，主要讲讲 NLP 的概念和研究范畴，以及当前的进展。 什么是 NLP？课程首先给出了对于 NLP 的定义，即 NLP 是一门包含计算机科学、人工智能以及语言学的交叉学科。 NLP 的目标则是想让计算机去处理或者说是“理解”自然语言以完成有意义的任务。但是必须明确的是，以目前的研究进展来看，想要完全理解并表示出语言的意思是一个十分困难的目标，完美的语言理解就可以同强 AI 画上等号了。 以某交赵某老师的说法就是 ”NLP 是 AI 领域里的高级任务，如果有什么新的方法，先拿去其他领域那边做个试验，假如行得通的话，再来试试这边高级的任务效果如何。“ 其实也很好理解，自然语言本身就是人类文明的一种抽象表达，是基于人类认知而产生的高层认知抽象实体，相较于 CV 和 Speech 那种底层的原始输入信号而言，更加难以处理。 NLP 的层次 上述这张很好地介绍了 NLP 领域的几个大方向，最顶层即为 NLP 的两个输入来源， speech 和 text，因而需要专门的语音分析或者 OCR 等工具。 接着就是对输入进行形态学的分析。这里有必要列一下形态学的概念： 形态学（morphology）：形态学（又称“词汇形态学”或“词法”）是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两个部分。由于词具有语音特征、句法特征和语义特征，形态学处于音位学、句法学和语义学的结合部位，所以形态学是每个语言学家都要关注的一门学科［Matthews,2000］。 紧接着就是本门课的重头戏，句法分析和语义分析。 最后则是话语级别层面上的分析了，这里就需要更好地理解全文信息和上下文信息，NLU 也是目前 NLP 领域炙手可热的一个方向。 NLP 的应用从简单到复杂地来看看 NLP 的应用场景： 拼写检查、关键词搜索、发现同义词 信息检索、文本分类 机器翻译 对话系统 复杂的问答系统 NLP 在工业届已经起飞NLP 在工业届的应用范围也很广，包括了： 搜索 在线广告投放 自动 / 辅助翻译 情感分析 语音识别 聊天机器人 人类语言的特殊之处人类语言是为了传递说话者 / 写作者的意思而特别构建的一套系统，这不仅只是一种环境信号而是一种刻意的沟通，并且最神奇的是，它使用的是一种简单到小屁孩都可以快速学会的编码方式。 人类的语言是一种离散的、具有象征性的、明确的信号系统。 人类语言可以用声音、手势、图像/文字来编码，及以它们作为载体，但是不同载体所包含的意思却是一样的。 虽然人类语言是明确的符号系统，但符号传输到大脑的过程是通过连续的声学光学信号，大脑编码似乎是连续的激活值上的模式。另外巨大的词表也导致数据稀疏，不利于机器学习。这构成一种动机，是不是应该用连续的信号而不是离散的符号去处理语言 (这就是后续课程会涉及的连续词向量表示了)。 什么是深度学习？深度学习是机器学习的一个子集。 大多数机器学习方法之所以能够有效是因为其使用了一些精心挑选的输入特征以及人类手工设计出来的特征。下图就表示了机器学习的尴尬之处，生动诠释了什么叫”人工“智能。 而深度学习则被归入到表示学习中去，它试图自动地去学习那些合适的特征或表示方式。这就是我们为什么要研究深度学习的原因了。 虽然深度学习有很多优势，但它却是在 2010 年后才开始优于其他机器学习的技术的，为什么是到了这个时间呢？原因有以下几个： 大量的数据集的出现 多核 CPU / GPU 硬件的支持 新的模型、算法、思想的出现 更好更灵活的中间层表示的学习 有效的 end-to-end 系统 利用了上下文信息、任务间迁移信息的有效学习算法 这轮深度学习引发的改革基本上是从 Speech 和 CV 先开始的，最后才波及到难度更大的 NLP 领域。 以下分别是 Speech 和 CV 领域 DL 的突破性论文。 [Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition Dahl et al. (2010) ] [ImageNet Classification with Deep Convolutional Neural Networks by Krizhevsky, Sutskever, &amp; Hinton, 2012, U. Toronto. ] 为什么 NLP 很难？ 使用语言学、实际情景、现实世界、视觉信息来对自然语言进行表示和学习本身的难度就很大了。 人类语言是含有歧义的。 人类语言的解读依赖于现实世界、常识以及上下文。 接下来老爷子讲了几个关于歧义的段子，然而对于非土著的我而言很难 get 到笑点。 Deep NLP = Deep Learning + NLP将 NLP 的思想与表示学习结合起来，并用深度学习的方法解决 NLP 目标。这种方式在近几年提高了 NLP 领域许多方面的效果： 层次：语音、词汇、语法、语义 工具：词性标注、命名实体识别、句法 / 语义分析 应用：机器翻译、情感分析、对话系统、问答系统 接下来的最后 15 分钟左右时间则是介绍了一下以上 NLP 各个领域深度学习所带来的改变。 词向量：离散的 one-hot、共现矩阵 -&gt; 连续分布式向量表示 形态学：词素 (e.g. prefix, stem, suffix, un, interest, ed) -&gt; 将词素也作为向量 (e.g. subword) 句法分析 / 语义分析：规则 -&gt; 直接上神经网络 (e.g. RNN, CNN, Tree RNN) 情感分析：人工搜集词典，在词袋模型上做分类 -&gt; 依旧是神经网络 问答 / 对话系统：手工编写大量规则，然后做 IR -&gt; 直接使用深度学习框架 机器翻译：从词语、语法、语义等不同层级的信息入手，期望找到一种通用的 Interlingua 来作为各种语言交互的桥梁 -&gt; emmmm…依旧是安利深度学习。 至此，本门课算是带领大家了解了一下 DLNLP 最近几年在各个方向的发展，还算可以轻松的水过这节课，但接下来的课程就开始劝退模式了。 参考文献 cs224n lecture1 cs224n lecture1 slides CS224n笔记1 自然语言处理与深度学习简介]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intro]]></title>
    <url>%2F2018%2F02%2F01%2FIntro%2F</url>
    <content type="text"><![CDATA[-写作动机- 一直有写博客的想法，却一直都只是想想而已。 总觉得维护博客是一件很花费时间的事情，也担心自己水平不够，没什么好写的。 今天总算跨出了这一步，花了点时间配置了一下个人博客，也想好了该去写一些什么东西，剩下的就是希望自己能够一直坚持写下去。 阐述一下自己写这个博客的动机吧： 克服惰性 巩固学习 帮助自己和他人 提高写作、表达水平 记录经历 希望几年后回首时，能够感谢自己现在迈出的这一步。]]></content>
      <categories>
        <category>By-Talk</category>
      </categories>
      <tags>
        <tag>By-Talk</tag>
      </tags>
  </entry>
</search>
