<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[A review on word embedding]]></title>
    <url>%2F2018%2F07%2F09%2FA-review-on-word-embedding%2F</url>
    <content type="text"><![CDATA[这是我第一次写回顾总结性质的文章，真正坐下来开写的时候，才意识到想把一件事物按其历史发展总结展望一下的难度还是蛮大的。 因为明天得做一个组会的 presentation，就想着把我所了解的 embedding 方法给做一个整体性的介绍，希望能够对后续继续入生物序列词嵌入坑的师弟师妹们有所帮助。 文章结构就分成 5 大块，分别为单词级别、句子 / 文档级别、子词级别、字符级别的 embedding 方法，以及最后的总结和展望。 Word-level Embedding单词级别的 embedding 方法之前就有几篇文章写过了，CS224n Lecture 2，CS224n Lecture 3，CS224n Research Highlight 3。 在此再简单提一下单词级别的 embedding 方法的发展。 传统方式最初 NLP 领域是靠一个大词典 (例如WordNet) ，所使用的是上位词和同义词集的信息来表示词的意思。但是这种方法忽视了单词的语境，并且很难维护。 NLP 领域还用 One-hot 编码方式来表示词的意思，但是这种方法的缺点也是显而易见的，即无法度量单词之间的相似度、数据稀疏、维度灾难。 稠密词向量然后 NLP 专家们想出了用稠密的向量来表示一个词的意思，这里需要特别提一下的是一句话 You shall know a word by the company it keeps. 换句话说就是一个词的意思可以由其上下文来表示，这种观点是后续词嵌入模型的根基。 最初模型走的路子大概可以分为两条：直接使用局部上下文信息的方法、基于共现矩阵的方法。 前者的代表为 Word2Vec 算法，后者的代表为 GloVe 算法，具体的内容可以参照以上三篇。 动态方法 ELMo今年在 NAACL18 上 ELMo 横空出世了，ELMo 词向量的使用把各种 NLP 任务的 state-of-the-art 刷新了一下。 优势ELMo 的优势如下： 能够学习到词汇用法的复杂性，比如语法、语义。 能够学习不同上下文情况下的词汇多义性。 与上述的几种方法不同的是，ELMo 所学得的词向量是动态的，在不同的上下文环境中将会得到不同的词向量表达。 定义ELMo 的思路也是利用单词的上下文信息来表示中心词，与 Word2Vec 等方法简单的线性模型不同的是，ELMo 所用的为 biLSTM 模型，公式如下： 进而最大化其似然函数可得目标函数为： 模型细节ELMo 是双向语言模型 biLM 的多层表示的组合，对于某一个词语 $t_k$，一个 L 层的双向语言模型 biLM 能够由 2L+1 个向量表示。 ELMo 将多层的 biLM 的输出 R 整合成一个向量，$ELMo_k = E(R_k;\theta_e)$。不同层的隐藏状态保留了不同层次的单词信息，一种比较简单的方法是直接拿最顶层的隐藏状态作为词向量，而最好的方法则是将 biLM 层所有层的输出加上一个正则化的 softmax 得到的权重向量。 其中 $\gamma$ 是缩放因子，作用类似于 LN。 论文里头没有给出模型图，因为时间关系，我就随便上网扒了一张 ELMo 的模型图，不过我发现这图是有错误的，姑且放上来凑个数。论文中对模型的描述为 The ﬁnal model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the ﬁrst to second layer. 这里需要说明的是，ELMo 的输入为 char-n-gram embedding，来自 CNN + highway network。 The context insensitive type representation uses 2048 character n-gram convolutional ﬁlters followed by two highway layers and a linear projection down to a 512 representation. 作者发现 ELMo 模型如果能够进行适当的 dropout 或者加入 L2 范式的话，可以使得其最终权重保持在各层 biLM 层的权重均值附近。 用法ELMo 的使用方法也是比较有意思的，有以下两种： 直接将 ELMo 词向量与普通词向量拼接。 直接将 ELMo 词向量与隐藏变量拼接。 Sentence / Document-level Embedding在句子和文档层面上，由于句子和文档与单词不同，出现的次数很少，并没有像单词一样预训练出词向量以供使用的必要，而是在特定的 NLP 任务中动态生成。 句子和文档层面的 embedding 方法主要分为两类：无监督方法和有监督方法。 无监督方法我在之前的 CS224n Research Highligh 1 介绍了一种简单有效的无监督方法。时间关系，这里留一个坑 Skip-thought vectors，Quick-thoughts vectors。 有监督方法以往的有监督方法只是通过简单的 RNN、CNN 架构来实现，效果往往比无监督方式差，但是最近提出的 InferSent 则取得了非常好的效果，这篇文章是用来做自然语言推导 (NLI) 的，因为之后打算写一篇 NLI 方向的文章，所以这个模型打算放到那边再讲。 InferSent 的模型结构很简单，如下图所示。其编码器由 BiLSTM + max pooling 构成。 Subword-level Embedding因为一些单词出现 Char-level Embedding展望参考文献 深度 | 当前最好的词句嵌入技术概览：从无监督学习转向监督、多任务学习 ELMo ELMo 最好用的词向量《Deep Contextualized Word Representations》 Skip-thought vectors Quick-thoughts vectors InferSent Enriching Word Vectors with Subword Information]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
        <tag>Review</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 6 Dependency Parsing]]></title>
    <url>%2F2018%2F06%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-6-Dependency-Parsing%2F</url>
    <content type="text"><![CDATA[整理中，待上传…]]></content>
      <categories>
        <category>Course</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Research Highlight3 Bag of Tricks for Efficient Text Classification]]></title>
    <url>%2F2018%2F05%2F09%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20Research%20Highlight3%20Bag%20of%20Tricks%20for%20Efficient%20Text%20Classification%2F</url>
    <content type="text"><![CDATA[本文是 FAIR 在 ACL17 上面的一篇文章。目的是为了解决 NLP 中常见的文本分类问题，并且能够产生词向量作为副产物。 本文提出了大名鼎鼎的 fastText 工具，训练速度很快，而且效果可以跟深度神经网络相当。 本文的作者是 Mikolov，即也是 Word2Vec 的作者，这篇文章在模型架构上跟 Word2Vec 很相似，因为之前已经有详细写过 Word2Vec ，在此就简单说一下他们的区别。 模型架构本篇文章的方法部分大概可以分为三块 (虽然按标题看只有两段)：fastText 模型架构、Hierarchical softmax、N-gram features。 fastText 模型架构 fastText 的模型架构部分跟 CBOW 很相似，这里直接说区别： 区别 CBOW fastText 输入 中心词的上下文 多个单词及 n-gram 特征 编码方式 One-hot Embedding 输出 中心词 文档类别 fastText 的核心思想：将整篇文档的词及 n-gram 向量叠加平均得到文档向量，然后用 hierarchical softmax 做分类。 Hierarchical softmax其实这块没什么好说的，只是把 CBOW 进行的输出端中心词分类换成了对于文档的分类，依照文档的类别建 huffman 树。 N-gram features这块其实是一种很常见的方法，我们知道预训练所得的词向量是固定的，这在于文本分类的任务中，将会丢失一些句子 \ 文档环境中的上下文信息。于是思路就很自然地想到了用 N-gram 的方法来捕获一些上下文信息。 这里需要提及的是本文中使用的 n-gram 是词级别的，举个例子： 关于句子 “Bag of Tricks for Efficient Text Classification”，假设 n 取 3，则整个句子就变为 (“Bag of”, “Bag of Tricks”, “of Tricks for”, “Tricks for Efficient”, …)。 结果 直接上图，效果不错。fastText 宣扬了一下奥卡姆剃刀的思想，“杀鸡焉用牛刀”。不过得注意一下，fastText 的输入是 embedding 后的词向量，本身就包含了单词的相似度、语义、语法信息，这是其除了 N-gram 外提升精度的另一个原因。 本文其实并没有太多好讲的地方，不过跟其同阶段提出的 Enriching Word Vectors with Subword Information 很值得学习，这篇文章我在 A review on word embedding 进行了介绍。 参考文献 Bag of Tricks for Efficient Text Classification CS224n研究热点3 高效文本分类的锦囊妙计 专栏 | fastText原理及实践]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>cs224n</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 5 Backpropagation and Project Advice]]></title>
    <url>%2F2018%2F05%2F09%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-5-Backpropagation-and-Project-Advice%2F</url>
    <content type="text"><![CDATA[整理中，待上传…]]></content>
      <categories>
        <category>Course</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 4 Word Window Classification and Neural Network]]></title>
    <url>%2F2018%2F05%2F08%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-4-Word-Window-Classification-and-Neural-Network%2F</url>
    <content type="text"><![CDATA[整理中，待上传…]]></content>
      <categories>
        <category>Course</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 3 Advanced Word Vector Representations]]></title>
    <url>%2F2018%2F05%2F07%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-3-Advanced-Word-Vector-Representations%2F</url>
    <content type="text"><![CDATA[这节课一开始先回顾了一下上节课的内容，Word2Vec。 Word2Vec 总结接着对 Word2Vec 算法作了一个总结： Word2Vec 遍历了整个语料库中的每一个词。 Word2Vec 算法的主要思想为通过一个单词的上下文信息中得到这个单词的意思，即通过中心词来预测其上下文信息 (SG) 和通过上下文信息来预测中心词 (CBOW)。 Word2Vec 捕获的是共现词中是否同时出现的信息。 这里由第 3 点引出了一种新的思路，Word2Vec 算法只是捕捉了两个词是否同时出现的信息，那我们是不是可以直接去捕捉单词间共现次数的信息呢？很显然答案是肯定的，这也就是本节课的内容，GloVe。 基于共现矩阵的词嵌入模型在讲述 GloVe 算法之前，我们先讲一下较为原始的方法。首先我们需要通过大量的语料文本来构建一个共现矩阵 (Co-occurrence Matrix)。矩阵的构建方式有两种：document-based 和 windows based。 前者一般用于主题模型 (LSA)，由于统计的是全文的信息，所以这种矩阵很难描述单词的语法信息。后者类似于 Word2Vec，需要指定一个统计的窗口大小，只在窗口的范围内统计单词的共现次数，这种方法可以同时捕捉到语法信息 (POS) 和语义信息。 举个例子，假设语料由三句话组成： I like deep learning. I like NLP. I enjoy flying. 那么如果我们把其窗口的大小设为1，并使用对称窗口的方法，则其共现矩阵如下： counts I like enjoy deep learning NLP flying . I 0 2 1 0 0 0 0 0 like 2 0 0 1 0 1 0 0 enjoy 1 0 0 0 0 0 1 0 deep 0 1 0 0 1 0 0 0 learning 0 0 0 1 0 0 0 1 NLP 0 1 0 0 0 0 0 1 flying 0 0 1 0 0 0 0 1 . 0 0 0 0 1 1 1 0 如果直接简单的以共现向量当做单词的词向量的话，虽然不同的词向量之间就不再像 One-hot 一样是正交的了，可以一定程度上用来计算单词间的相似度，但是还是存在许多问题： 随着词表的增加，词向量的维度也得跟着增加。 维度灾难问题。语料足够大的时候词表也会很长，导致向量长度过大，训练的代价高昂。 数据稀疏。与 One-hot 类似，共现矩阵中不为 0 的维度数量很少，会使得后续的分类模型鲁棒性下降。 共现矩阵的降维处理为了解决以上问题，语言学家们自然而然地想到了将共现矩阵进行降维，进而得到单词的稠密表示 (dense representation)。 那么如何对共现矩阵进行降维呢？一个很常见的方法就是使用奇异值分解 (SVD)。SVD 的基本思想是，通过将原共现矩阵 X 分解为一个正交矩阵 U，一个对角矩阵 S，和另一个正交矩阵 V 乘积的形式，并提取 U 的 k 个主成分（按 S 里对角元的大小排序）构造低维词向量。具体原理推导可以参考 leftnoteasy的博文。 除此之外，对于共现矩阵 X 的处理也有很多值得一提的 Hacks： 部分的单词 (the, he, has) 出现的次数过多，使得它们对于语法信息的影响过大。改进的方法就是限制高频词的最大频次 (min(X, t), with t~100)，或者干脆直接停用高频词。 带权重地统计窗口。距离中心词跃进的词对于词义的贡献就越大。 使用 Pearson 相关系数来替代掉词频，并把负值置 0。 以上的方法虽然简单粗暴，但是还是取得了很不错的结果。 然而 SVD 算法有几个根本性的问题没法解决： SVD 是一个计算复杂度高昂 (O($mn^2$)) 的算法，无法在实际环境中使用。 SVD 方法不方便处理新词或者新的文档，如果加入一些新的语料后，就需要重新再进行 SVD。 SVD 的画风跟其他的 DL 模型截然不同。 基于共现次数统计的方法 VS 直接进行预测的方法讲完了 SVD 这类基于共现次数统计的方法和之前的 Word2Vec 这类直接进行预测的方法，课程中列出了以下这一张两种方法的对比图，两种方法的优势和劣势总结得十分到位。 那么有没有一种方法能够同时结合以上这两种方法的优势呢？这就轮到本课的主题 GloVe 出马了。 GloVe (Global Vectors)为了结合以上两种方法的优势，GloVe 模型既使用了语料库的全局统计特征 (全局共现次数统计)，也使用了局部的上下文特征 (窗口)。因而 GloVe 模型引入了共现概率矩阵 (Co-occurrence Probabilities Matrix)。 直接上论文中的例子： 该矩阵的第一个元素为 ice 出现时 solid 出现的概率，第二个元素为 ice 出现时 gas 出现的概率，以此类推。由共现概率矩阵的值可以看出，概率比率 ($F(w_i, w_j, \hat w_k) = \frac{p_{i,k}}{p_{j,k}}$) 的取值是有一定规律的。 规律总结如下： $F(w_i, w_j, \hat w_k)$ $w_j, w_k$ 相关 $w_j, w_k$ 不相关 $w_i, w_k$ 相关 趋近 1 很大 $w_i, w_k$ 不相关 很小 趋近 1 也就是说，与原本的概率值相比，概率比例能够更好地表示出两个单词间的相关关系。这是一个很简单但是很有用的规律，如果我们用词向量 $w_i, w_j, w_k$ 通过某种函数计算 $F(w_i, w_j, \hat w_k)$，能够同样得到这样的规律的话，就意味着我们词向量与共现矩阵具有很好的一致性，也就说明我们的词向量中蕴含了共现矩阵中所蕴含的信息。 于是接下来就到了原作者神奇的脑洞时间了。 先上公式：$F(w_i, w_j, \hat w_k) = \frac{p_{i,k}}{p_{j,k}}$ 这里等号的左端为全局统计求得的值，右端的 $w_i, w_j, w_k$ 就是我们要求得的词向量，而函数 F 是未知的，作者确定 F 的过程真是让人瞠目结舌。过程如下所示： $\frac{p_{i,k}}{p_{j,k}}$ 这个值考察了 $w_i, w_j, w_k$ 三个词两两之间的相关关系，但是这样很难进行 F 的求解，所以更好的方法是先去考察 $w_i, w_j$ 两个词之间的关系，线性空间中的相似性关系自然想到的是两个向量的差 $(w_i - w_j)$，所以我们可以把 F 的形式转化为 $F(w_i - w_j, w_k) = \frac{p_{i,k}}{p_{j,k}}$。 $\frac{p_{ik}}{p_{jk}}$ 是一个标量，而 F 是直接作用在 $w_i - w_j$ 和 $w_k$ 这两个向量上的，为了把向量转化为标量，自然地就想到了用内积的方法，所以我们可以把 F 的形式进一步转化为 $F((w_i - w_j)^T w_k) = F(w_i^Tw_k - w_j^Tw_k) = \frac{p_{i,k}}{p_{j,k}}$。 此时 F 的公式的形式为 $F(w_i^Tw_k - w_j^Tw_k) = \frac{p_{i,k}}{p_{j,k}}$。等号左边为差的形式，右边则是商的形式，要把差和商关联起来，作者又想到了用取指数的形式，即 $\exp(w_i^Tw_k - w_j^Tw_k) = \frac{\exp(w_i^Tw_k)}{\exp(w_j^Tw_k)} = \frac{p_{i,k}}{p_{j,k}}$。 现在形式就很明朗了，我们只需让 $\exp(w_i^Tw_k) = p_{i,k}, \exp(w_j^Tw_k) = p_{j,k}$ 等式就能够成立。 那么如何让 $\exp(w_i^Tw_k) = p_{i,k} = \frac{x_{i,k}}{x_i}$ 成立呢？只需让 $w_i^Twk = \log\frac{x_{i,k}}{x_i} = \log x_{i,k} - \log x_i$。 又因为作为向量，i 和 k 的顺序交换后 $w_i^Tw_k$ 和 $w_k^Tw_i$ 应该是相等的，即它们应该是对称的。但上式的右边显然不符合这个条件，所以为了解决这个问题，作者又引入了两个偏置项 $b_i, b_k$，这样模型就变成了 $\log x_{i,k} = w_i^Tw_k + b_i + b_k$，其中 $b_i$ 包含了 $\log x_i$。此外还加入了 $b_k$ 来保证模型的对称性。 因此，我们就可以得到 GloVe 的目标函数了：$J = \sum_{i,k}(w_i^Tw_k + b_i + b_k - \log x_{i,k})^2$。 再考虑到出现频率越高的词对权重的影响应该越大这个原则，我们需要在目标函数里加一个频率权重项 $f(x_{i, k})$，所以最终的目标函数为：$J = \sum_{ik}f(x_{i,k})(w_i^Tw_k + b_i + b_k - \log x_{i,k})^2$。 对于这个频率权重项 $f(x_{i, k})$ 我们需要确保其为非减的，并且类似于之前对 (the, he, has) 这类常见词的处理方式，当词频过高的时候，频率的权重项不应该过大，因而需要将 $f$ 控制在一个合适的范围，所以频率权重函数 $f$ 的公式如下： 最终作者经过实验得出当 $x_\max = 100, \alpha = 0.75$ 是一个比较好的选择。 以上就是 GloVe 算法的完整推导流程，这里可以看到，最终我们得到的是 $w_i, w_k$ 两个权重矩阵，它们都捕捉到了单词的共现信息，在实际使用中，实验证明直接将二者简单的相加，得到的权重矩阵就是效果最好的词向量表示，即 $w_{final} = w_i + w_k$。 GloVe 优点 训练速度快。 可以扩展到大规模的语料。 也适用于小规模语料和小向量。 如何评估词向量评估词向量质量高低的方法分为两种： Intrinsic (内部) 和 extrinsic (外部)。 IntrinsicIntrinsic 评估的方式为专门设计单独的试验，由人工标注词语或句子相似度，与模型结果对比。 这种方法看似比较合理，并且计算速度也很快，然而却不一定对实际应用有帮助。有时候你花很多时间来调整词向量，使得内部评估方式的得分看起来很高，结果在实际应用上一跑预测结果却下降了。(尴尬又不失礼貌的微笑.jpg) 对于词向量模型来说，一个常用的 Intrinsic 评估是向量类比 (word vector analogies)，它评估了一组词向量在语义和句法上表现出来的线性关系。 以上例而言，我们给定了一组词 $(a, b, c, d)$ 我们要验证的是 $d$ 与向量 $(x_b - x_a + x_c)$ 的余弦距离的值要最接近于 $d$ 这个词本身。 ExtrinsicIntrinsic 评估的方式为通过对外部实际应用的效果提升来体现。 耗时较长，不能排除是否是新的词向量与旧系统的某种契合度产生，需要至少两个subsystems同时证明。这类评测中，往往会用pre-train的向量在外部任务的语料上retrain。 词向量训练经验直接上结论： GloVe 的效果在一些任务上可能会比 Word2Vec 更好，但有些时候二者并没什么区别，由于 Word2Vec 出现得更早，原理也更简单易懂，因而目前为止使用得还是很广泛的。 语料质量 &gt; 语料数量。记得有某个 kaggle 的关于 Quora 句子相似度的比赛，我们组使用了个不错的模型，结果被其他组用普通的 LSTM 模型吊打得花枝乱颤，归其原因除了预处理没有做好外，我们使用的是泛用的语料训练得到的词向量，而那组人用的是专门的 Quora 语料。 向量维度、窗口大小、窗口是否对称对词向量质量也有影响，具体如图。然而这是人类语言中的效果图，在不同的领域中参数还是需要根据实际情况来调整的。比如我之前做的 RNA 序列问题，最终最好的结果所取的窗口大小大于图中所示，向量维度反而是小于的。 GloVe 相比 Word2Vec 更加稳定。 维基百科的语料好于新闻的语料。 在训练集大小较小的时候，在模型的训练过程对词向量进行 retrain 的话，会导致整个向量空间原有的几何结构被破坏，从而使得泛化能力变差，所以当训练集不够充分的时候不要 retrain 词向量。当训练集足够大的时候，retrain 词向量往往可以使预测精度得到提升。 从菜爸爸那里得知的一个 trick，可以同时使用两个 pretrain 的词向量，一组固定，一组随着模型的训练进行 retrain，可以使得精度提高。 参考文献 cs224n lecture3 cs224n lecture3 slide GloVe: Global Vectors for Word Representation 强大的矩阵奇异值分解(SVD)及其应用 理解 GloVe 模型 (Global vectors for word representation) GloVe模型 (Stanford CS224d) Deep Learning and NLP课程笔记（三）：GloVe与模型的评估 CS224n笔记3 高级词向量表示]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>cs224n</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding]]></title>
    <url>%2F2018%2F05%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20Research%20Highlight1%20A%20simple%20but%20tough-to-beat%20baseline%20for%20sentence%20embedding%2F</url>
    <content type="text"><![CDATA[cs224n 这门课很有意思的一个地方在于教授会让 TA 在中场休息时候花个 5 分钟左右的时间来讲一下当前的研究亮点。我觉得这点很可取，这么做有助于学生开拓思路、紧跟当下热点，可惜这种做法在国内的大学中是很少能够看见。 这个小讲座讲的是发表在 ICLR17 的一篇文章，A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS。 动机本文是用无监督方法做句子级别的 embedding，用的是一个十分简单但却又很有效的传统方法，这在神经网络泛滥的年代算是一股清流了。 作者做 word2sen 的动机是想要得到句子的词向量，这样就可以进行句子间的相似度计算了。除此之外，还能在这些句向量的基础上构建分类器来做情感分析的研究。 已有方法稍微介绍一下目前已有的方法。 基于无监督的线性变换的方法。例如简单的对词向量求平均，或者对词向量进行加权平均 (例如以 TF-IDF 为权值)。 基于有监督的神经网络的方法。例如各种满天乱飞的 CNN, RNN (Recurrent), RNN (Recursive) 模型得到的句向量。 本文方法作者将该算法称为 WR，W 表示 Weighted，根据预设的超参数和词频给每个词向量赋予权重。R 表示 Removal，使用PCA移除句向量中的无关部分。 算法流程分为两步： W 步：对于句子中的每个词向量乘以一个独特的权值，即 $\frac{a}{a+p(w)}$，其中 $a$ 为一个常数 (论文中建议 $a$ 的范围： [$1e−4,1e−3$] )， $p(w)$ 为该词的频率。 R 步：计算语料库所有句向量构成的矩阵的第一个主成分 $u$，让每个句向量减去它在 $u$ 上的投影 (类似 PCA)。其中，一个向量 $v$ 在另一个向量 $u$ 上的投影定义为：$Proj_uv = \frac{uu^Tv}{||u||^2}$。 概率论解释 其原理是，给定上下文向量，一个词的出现概率由两项决定：作为平滑项的词频，以及上下文。其中第二项的意思是，有一个平滑变动的上下文随机地发射单词。 这篇文章我没有看过原文，这里其实并不是完全理解，等之后再来填坑。 结果 取得了一个不错的结果，但是比 LSTM 效果好这种说法不太妥当，这得取决于实际的任务。 不过无论如何，这种方法运行耗时短，结果又不错，可以当做一个很好的 baseline。 参考文献 A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SEN- TENCE EMBEDDINGS CS224n研究热点1 一个简单但很难超越的Sentence Embedding基线方法]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>cs224n</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 2 Word Vector Representations word2vec]]></title>
    <url>%2F2018%2F05%2F06%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-2-Word-Vector-Representations-word2vec%2F</url>
    <content type="text"><![CDATA[之前的第一课里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。 对于这个现象，有一种说法是： 语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。 这里就涉及了深度学习中输入的问题，也是所有 NLP 任务中都无法绕过的问题，即在计算机里如何表示一个词？ 答案显而易见，就是本节课的主题词向量 (Word Vector)。为了方便理解，我补充了一些课程中没有讲到的内容。 如何表示词的意思姑且先看看 Webster 词典中对于 meaning 的定义： the idea that is represented by a word, phrase, etc. the idea that a person wants to express by using words, signs, etc. the idea that is expressed in a word of writing, art, etc. 而在语言学中，对于 meaning最常见的解释为： signifier ⟺ signified (idea or thing) = denotation 在计算机中我们该如何得到可用的词意以上的定义由具体到抽象，在 NLP 领域中，主要是针对第一条 word representation进行研究。 最初 NLP 领域使用的是分类系统来对词的意思进行表示，比如 WordNet。我们可以通过查询单词的上位词以及同义词集来得知这个词的意思。￼ 比如上图，我们查询 panda 的上位词，可以得到 procyonid (浣熊科)、carnivore (肉食动物) 等词。查询 good 的同义词，可以得到 full、 honorable、 beneficial、ripe 等词。 这就是一种离散的表示 (Discrete Representation)，然而这种表示方式存在很多的问题： 词语有着独特的语境。以上面的同义词为例，在查询 good 的时候我们发现， ripe 和 honorable 都为其同义词，但是我们在描述一件事物为 good 的时候，如果使用其同义词 ripe 或者 honorable 来代替的话，很多时候都会显得很别扭。 语言的变化日新月异，每天都有大量的新词涌现出来，而使用这种表示方式的话，很难及时更新旧词的含义和对新词进行定义。维护一个类似 WordNet 这样的数据库是需要花费大量的人力的，代价非常高昂。并且，使用人力去维护词库的话，必不可免地会存在人的主观性的影响。 通过这种方式，我们能够得到的只是词的上下位词的信息，以及其同义词集信息，这种信息都只是一个二分类的结果，导致我们很难对于两个词之间的相似度给出一个准确的度量方式。 在绝大多数的基于规则和基于统计的 NLP 工作中，都将单词视为一种原子符号来进行处理。 比较常见的做法就是用 one-hot 向量来表示一个词，这是一种 localist representation。 从 symbolic representations 到 distributed representationsOne-hot 看着是一种简单粗暴又可行的表示方法，然而在实际使用中，像 one-hot 这种离散的 symbolic representations 是存在一些问题的： One-hot 向量是正交的，这种表示方法是无法得到不同单词间的相似度的。 One-hot 向量存在着数据稀疏的问题。 因其表示的方式的问题，随着所用词表的扩增，One-hot 向量还会出现维度灾难。(Dimensionality: 20K (speech) – 50K (PTB) – 500K (big vocab) – 13M (Google 1T) 因而，我们需要去探讨一种新的词向量的编码方式。 You shall know a word by the company it keeps. – J. R. Firth 1957: 11 这里就提出了一种思路，即我们可以通过一个单词的上下文信息中得到这个单词的意思。 这是现代统计自然语言处理最成功的思想之一，事实上这也是 Word2Vec 算法的根基。 Word2Vec定义首先我们需要先定义一下预测单词上下文的模型及损失函数： $p(context | w_t) = …$ $J = 1 - p(w_{-t} | w_t)$ 这里的 $w_{-t}$ 表示除了 t 之外的上下文。 Word2Vec 的主要思想Word2Vec 有两种计算模型，两种模型的思路是截然相反的： Skip-grams (SG) : 通过中心词来预测其上下文信息 Continuous Bag of Words (CBOW) : 通过上下文信息来预测中心词 先上一张课件中 SG 模型的预测图方便大家理解，这里我们就是通过 banking 这个中心词来对其上下文信息进行预测，学习的目的就是要最大化上下文中各词的条件概率。 这里的上下文的条件概率 $p(w_{-t}|w_t)$ 是由 softmax 得到的： 然后，目标函数定义为所有位置的预测结果的乘积： 为了方便计算，加个负对数得到最终的损失函数，这样问题就转化为最小化损失函数了： 接下来就是上几张大家不知道见过多少遍的经典模型图了，以下分别为 CBOW 模型和 SG 模型： 模型流程这里参考知乎上crystlajj的回答，并偷懒盗用了不少图，以 CBOW 模型的流程为例： 输入层：上下文单词的 onehot. {假设单词向量空间dim为V，上下文单词个数为C} 输出层的 label ：中心词的 onehot。 权重矩阵 W 和 W’：维度分别为 {V*N} 和 {N*V}，并将权重矩阵初始化。要注意最后得到的输入端的 W 即为单词的词向量表示，词向量的维度为N（每一列表示一个词的词向量）。 所有上下文单词的 onehot 分别乘以共享的输入权重矩阵 W。 将每个单词的 onthot 与 W 相乘所得的 {1*N} 维向量相加求平均作为隐层向量。并将得到乘以输出权重矩阵W’，得到一个 {1*V} 维的向量。 将得到的 {1*V} 维向量经过 softmax 函数处理得到 V-dim 概率分布，概率最大的index所指示的单词为预测出的中心词（target word）。 与true label的onehot做比较，损失函数越小越好。 再偷几张图举个例子，让大家跟着走一遍CBOW模型的流程： 以上就是CBOW 模型的流程，这里再附上一张课件中给出的 SG 模型的流程，可以对比一下。 以上就是 Word2Vec 的两种模型的算法流程。其实这种 distributed representation 的思想其实很早就有了，而真正火起来是在 13 年 Mikolov 大神的两篇文章之后。 因为这个方法实际上存在着一些问题。比如训练词向量模型时我们输入端和输出端用到的依旧是 onehot 编码，维度灾难问题仍然存在。如果语料足够大的话，onehot 编码的长度就很大，每轮学习需要更新大量的参数，将会导致训练速度慢到难以接受的地步，如果语料不够充足的话，则很难学到高质量的词向量表达。 Word2Vec 优化因而 Mikolov 大神就提出了两种高效的训练方法： Hierarchical softmax Negative sampling 这部分是课程里头没有涉及的内容，并且公式太多太繁杂，在这里只做一个大概的介绍（实在是懒得写了 doge），具体的推导和源码解析可以参考 falao_beiliu 和 Hankcs 的文章。 Hierarchical softmax以 CBOW 为例，HS 的方法就是在输出层使用一颗 huffman 树来提升训练效率，模型图如下所示： Huffman 树在这里就不科普了，HS 方法中的 huffman 树的每个叶子结点表示一个单词，可以被 01 唯一地编码。每个非叶子结点相当于一个神经元。 由于 huffman 树的特点可以使得编码长度最短，这里我们将单词以词频来建立huffman 树，就可以使得常用词的路径较短，非常用词的路径较长。HS 方法就可以将原本查找目标词的多分类问题转化为多个二分类问题，在每一个非叶子结点进行 softmax 运算，根据结果选择向左下走或者向右下走，进而直到走到叶子结点，就可以得知预测的目标词是什么。通过这种方法，我们每次更新的只是走过的路径上的参数，而非全部的参数，可以使得效率大幅度提高。 SG 模型的 CBOW 方法同理，结构示意图如下所示： Negative Sampling回想一下之前的算法，我们输出的 label 都是一个词的 one-hot 编码，维度为整个词库的大小，这里的目标词的 index 即为分类任务的正例，其他的词都为负例，也就是说在一个单词数为 N 的词库中，我们的分类任务的正例为 1，负例为 N - 1。正常 N 值会远远大于 1，但是每次更新中，所有的负例的参数权重也会跟着一块更新，在 Mikolov 看来这其实是一个很没有效率的事情，我们大可以把负例减少来提高效率并且对于最后的结果不会产生大的影响。 因此就提出了 NS 方法，顾名思义，NS 即随机挑选出一些负例。采样的算法思想都应该保证频次越高的样本越容易被采集到，因此，NS 算法基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语： $len(w) = \frac{cnt(w)^N}{\sum_{u\epsilon D}cnt(u)^N}$ 所以就可以根据词频来分配线段中每个词所占有的长度了。这里 Word2Vec 为了提高效率，用了点小 trick，即将原本线段标上 M 个刻度，这样就不需要浮点数的操作，而是直接使用 0-M 间的整数，经过查表就可以得知该取哪个单词了。 注意这个 len(w) 的计算有包含一个幂计算 (N)，这实际上是一种平滑的策略，可以增大低频词的值，使之更容易被采样到。 以上大概就是本节课的内容和扩展，实际上课程里头还包含了一些数学知识的复习和 sgd 算法的推导，觉得过于老生常谈，就不拿出来说了，之后如果有相似的内容也会一样跳过。 参考文献 cs224n lecture2 cs224n lecture2 slides cs224n笔记2 词的向量表示 word2vec原理推导与代码分析 深度学习word2vec笔记之算法篇]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>cs224n</tag>
        <tag>NLP</tag>
        <tag>Word Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224n学习笔记 Lecture 1 Introduction to NLP and Deep Learning]]></title>
    <url>%2F2018%2F05%2F03%2FCS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-1-Introduction-to-NLP-and-Deep-Learning%2F</url>
    <content type="text"><![CDATA[整理中，待上传…]]></content>
      <categories>
        <category>Course</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Intro]]></title>
    <url>%2F2018%2F02%2F01%2FIntro%2F</url>
    <content type="text"><![CDATA[-写作动机- 一直有写博客的想法，却一直都只是想想而已。 总觉得维护博客是一件很花费时间的事情，也担心自己水平不够，没什么好写的。 今天总算跨出了这一步，花了点时间配置了一下个人博客，也想好了该去写一些什么东西，剩下的就是希望自己能够一直坚持写下去。 阐述一下自己写这个博客的动机吧： 克服惰性 巩固学习 帮助自己和他人 提高写作、表达水平 记录经历 希望几年后回首时，能够感谢自己现在迈出的这一步。]]></content>
      <categories>
        <category>By-Talk</category>
      </categories>
      <tags>
        <tag>By-Talk</tag>
      </tags>
  </entry>
</search>
