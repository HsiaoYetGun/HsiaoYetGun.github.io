<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>CS224n学习笔记 Lecture 2 Word Vector Representations word2vec | 如果没有人看着我</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="之前的第一课里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。">
<meta name="keywords" content="NLP,Word Embedding,CS224n">
<meta property="og:type" content="article">
<meta property="og:title" content="CS224n学习笔记 Lecture 2 Word Vector Representations word2vec">
<meta property="og:url" content="http://hsiaoyetgun.github.io/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/index.html">
<meta property="og:site_name" content="如果没有人看着我">
<meta property="og:description" content="之前的第一课里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/1531029768882.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/1531032299799.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/1531032696955.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/cs224n-2017-lecture2.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/006Fmjmcly1fgcnjx5gssj30tu09u47t.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/006Fmjmcly1fgcn8dsnw5j318q08m41y.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/006Fmjmcly1fgcn9ndo8dj316s092jwj.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/6cbb8645gw1f5to6e5d9lj216c0qkwhk.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-0f439e1bb44c71c8e694cc65cb509263_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-3e75211b3b675f17a232f29fae0982bc_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-abd3c7d6bc76c01266e8ddd32acfe31a_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-66655880a87789eaba5dd6f5c5033e94_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-5325f4a5d1fbacefd93ccb138b706a69_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/v2-1713450fa2a0f37c8cbcce4ffef04baa_hd.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/sg.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/6cbb8645gw1f5wmy4jdnwj214w12a42v.jpg">
<meta property="og:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/6cbb8645gw1f5wqzg68u0j214a120wij.jpg">
<meta property="og:updated_time" content="2018-07-23T18:12:11.001Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS224n学习笔记 Lecture 2 Word Vector Representations word2vec">
<meta name="twitter:description" content="之前的第一课里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。">
<meta name="twitter:image" content="http://hsiaoyetgun.github.io/picture/cs224n/lec2/1531029768882.jpg">
  
    <link rel="alternative" href="/atom.xml" title="如果没有人看着我" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  
      <link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">
  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          rootUrl: '/',
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            
            <img lazy-src="/img/head.jpg" class="js-avatar">
            
        </a>

        <hgroup>
          <h1 class="header-author"><a href="/" title="Hi Mate">Hsiao</a></h1>
        </hgroup>

        
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>Home</li>
                        <li>Tags</li>
                        
                        <li>Friends</li>
                        
                        
                        <li>About Me</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">Home</a></li>
                        
                            <li><a href="/archives">Archives</a></li>
                        
                            <li><a href="/aboutme">About Me</a></li>
                        
                            <li><a href="/tags">Tags</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl mail" target="_self" href="mailto:hsiaoyetgun@gmail.com" title="mail">mail</a>
                            
                                <a class="fl github" target="_self" href="https://github.com/hsiaoyetgun" title="github">github</a>
                            
                                <a class="fl zhihu" target="_self" href="https://www.zhihu.com/people/yetgun-hsiao/" title="zhihu">zhihu</a>
                            
                                <a class="fl linkedin" target="_self" href="#" title="linkedin">linkedin</a>
                            
                                <a class="fl pinterest" target="_self" href="http://music.163.com/#/user/home?id=61393493" title="pinterest">pinterest</a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/Ambiguity/" style="font-size: 10px;">Ambiguity</a> <a href="/tags/BP/" style="font-size: 10px;">BP</a> <a href="/tags/By-Talk/" style="font-size: 10px;">By-Talk</a> <a href="/tags/CS224n/" style="font-size: 16.67px;">CS224n</a> <a href="/tags/Deep-Learning/" style="font-size: 10px;">Deep Learning</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/Polysemy/" style="font-size: 10px;">Polysemy</a> <a href="/tags/Review/" style="font-size: 10px;">Review</a> <a href="/tags/Word-Embedding/" style="font-size: 13.33px;">Word Embedding</a>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_self" class="main-nav-link switch-friends-link" href="https://jiaxuncai.github.io">菜得二逼</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">人工智障，深度瞎学，NLP， Bioinformatics</div>
                </section>
                
            </div>
        </div>
    </header>

    <div style="bottom:120px left:auto; width:20px">
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=52 src="//music.163.com/outchain/player?type=2&id=28315773&auto=1&height=32"></iframe>
    </div>             
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="Me">Hsiao</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/head.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="Me">Hsiao</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">Home</a></li>
                
                    <li><a href="/archives">Archives</a></li>
                
                    <li><a href="/aboutme">About Me</a></li>
                
                    <li><a href="/tags">Tags</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="mail" target="_self" href="mailto:hsiaoyetgun@gmail.com" title="mail">mail</a>
                    
                        <a class="github" target="_self" href="https://github.com/hsiaoyetgun" title="github">github</a>
                    
                        <a class="zhihu" target="_self" href="https://www.zhihu.com/people/yetgun-hsiao/" title="zhihu">zhihu</a>
                    
                        <a class="linkedin" target="_self" href="#" title="linkedin">linkedin</a>
                    
                        <a class="pinterest" target="_self" href="http://music.163.com/#/user/home?id=61393493" title="pinterest">pinterest</a>
                    
                </div>
            </nav>
        </header>                
    </div>
</nav>
      <div class="body-wrap"><article id="post-CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/" class="article-date">
      <time datetime="2018-05-05T18:12:25.000Z" itemprop="datePublished">2018-05-06</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      CS224n学习笔记 Lecture 2 Word Vector Representations word2vec
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/Course/">Course</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CS224n/">CS224n</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word-Embedding/">Word Embedding</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>之前的<a href="https://hsiaoyetgun.github.io/2018/05/03/CS224n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Lecture-1-Introduction-to-NLP-and-Deep-Learning/">第一课</a>里头提及了为什么 NLP 任务比较难，Deep Learning 在 NLP 领域中取得的成果也不如 CV 和 Speech 来得令人惊讶。</p>
<a id="more"></a>
<p>对于这个现象，有一种说法是：</p>
<blockquote>
<p>语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。</p>
</blockquote>
<p>这里就涉及了深度学习中输入的问题，也是所有 NLP 任务中都无法绕过的问题，即<strong><code>在计算机里如何表示一个词？</code></strong></p>
<p>答案显而易见，就是本节课的主题<strong><code>词向量 (Word Vector)</code></strong>。为了方便理解，我补充了一些课程中没有讲到的内容。</p>
<h1 id="如何表示词的意思"><a href="#如何表示词的意思" class="headerlink" title="如何表示词的意思"></a>如何表示词的意思</h1><p>姑且先看看 Webster 词典中对于 <code>meaning</code> 的定义：</p>
<blockquote>
<ol>
<li>the idea that is represented by a word, phrase, etc.</li>
<li>the idea that a person wants to express by using words, signs, etc.</li>
<li>the idea that is expressed in a word of writing, art, etc.</li>
</ol>
</blockquote>
<p>而在语言学中，对于 <code>meaning</code>最常见的解释为：</p>
<blockquote>
<p>signifier ⟺ signified (idea or thing) = denotation</p>
</blockquote>
<h1 id="在计算机中我们该如何得到可用的词意"><a href="#在计算机中我们该如何得到可用的词意" class="headerlink" title="在计算机中我们该如何得到可用的词意"></a>在计算机中我们该如何得到可用的词意</h1><p>以上的定义由具体到抽象，在 NLP 领域中，主要是针对第一条 <code>word representation</code>进行研究。</p>
<p>最初 NLP 领域使用的是分类系统来对词的意思进行表示，比如 WordNet。我们可以通过查询单词的<a href="https://en.wikipedia.org/wiki/Hyponymy_and_hypernymy" target="_blank" rel="noopener">上位词</a>以及同义词集来得知这个词的意思。￼</p>
<p><img src="/picture/cs224n/lec2/1531029768882.jpg" alt="1531029768882"></p>
<p>比如上图，我们查询 panda 的上位词，可以得到 procyonid (浣熊科)、carnivore (肉食动物) 等词。查询 good 的同义词，可以得到 full、 honorable、 beneficial、ripe 等词。</p>
<p>这就是一种离散的表示 (Discrete Representation)，然而这种表示方式存在很多的问题：</p>
<ol>
<li>词语有着独特的语境。以上面的同义词为例，在查询 good 的时候我们发现， ripe 和 honorable 都为其同义词，但是我们在描述一件事物为 good 的时候，如果使用其同义词 ripe 或者 honorable 来代替的话，很多时候都会显得很别扭。</li>
<li>语言的变化日新月异，每天都有大量的新词涌现出来，而使用这种表示方式的话，很难及时更新旧词的含义和对新词进行定义。维护一个类似 WordNet 这样的数据库是需要花费大量的人力的，代价非常高昂。并且，使用人力去维护词库的话，必不可免地会存在人的主观性的影响。</li>
<li>通过这种方式，我们能够得到的只是词的上下位词的信息，以及其同义词集信息，这种信息都只是一个二分类的结果，导致我们很难对于两个词之间的相似度给出一个准确的度量方式。</li>
</ol>
<p>在绝大多数的基于规则和基于统计的 NLP 工作中，都将单词视为一种原子符号来进行处理。</p>
<p>比较常见的做法就是用 one-hot 向量来表示一个词，这是一种 localist representation。</p>
<h1 id="从-symbolic-representations-到-distributed-representations"><a href="#从-symbolic-representations-到-distributed-representations" class="headerlink" title="从 symbolic representations 到 distributed representations"></a>从 symbolic representations 到 distributed representations</h1><p>One-hot 看着是一种简单粗暴又可行的表示方法，然而在实际使用中，像 one-hot 这种离散的 symbolic representations 是存在一些问题的：</p>
<ol>
<li>One-hot 向量是正交的，这种表示方法是无法得到不同单词间的相似度的。<img src="/picture/cs224n/lec2/1531032299799.jpg" alt="1531032299799"></li>
<li>One-hot 向量存在着数据稀疏的问题。</li>
<li>因其表示的方式的问题，随着所用词表的扩增，One-hot 向量还会出现维度灾难。(Dimensionality: 20K (speech) – 50K (PTB) – 500K (big vocab) – 13M (Google 1T) </li>
</ol>
<p>因而，我们需要去探讨一种新的词向量的编码方式。</p>
<blockquote>
<p>You shall know a word by the company it keeps.</p>
<p>– J. R. Firth 1957: 11 </p>
</blockquote>
<p>这里就提出了一种思路，即我们可以<strong>通过一个单词的上下文信息中得到这个单词的意思。</strong></p>
<p><img src="/picture/cs224n/lec2/1531032696955.jpg" alt="1531032696955"></p>
<p>这是现代统计自然语言处理最成功的思想之一，事实上这也是 Word2Vec 算法的根基。</p>
<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>首先我们需要先定义一下预测单词上下文的模型及损失函数：</p>
<p>$p(context | w_t) = …$</p>
<p>$J = 1 - p(w_{-t} | w_t)$</p>
<p>这里的 $w_{-t}$ 表示除了 t 之外的上下文。</p>
<h2 id="Word2Vec-的主要思想"><a href="#Word2Vec-的主要思想" class="headerlink" title="Word2Vec 的主要思想"></a>Word2Vec 的主要思想</h2><p>Word2Vec 有两种计算模型，两种模型的思路是截然相反的：</p>
<ol>
<li>Skip-grams (SG) : 通过中心词来预测其上下文信息</li>
<li>Continuous Bag of Words (CBOW) : 通过上下文信息来预测中心词</li>
</ol>
<p>先上一张课件中 SG 模型的预测图方便大家理解，这里我们就是通过 banking 这个中心词来对其上下文信息进行预测，学习的目的就是要最大化上下文中各词的条件概率。</p>
<p><img src="/picture/cs224n/lec2/cs224n-2017-lecture2.jpg" alt="cs224n-2017-lecture2"></p>
<p>这里的上下文的条件概率 $p(w_{-t}|w_t)$ 是由 softmax 得到的：<img src="/picture/cs224n/lec2/006Fmjmcly1fgcnjx5gssj30tu09u47t.jpg" alt="006Fmjmcly1fgcnjx5gssj30tu09u47t"></p>
<p>然后，目标函数定义为所有位置的预测结果的乘积：<img src="/picture/cs224n/lec2/006Fmjmcly1fgcn8dsnw5j318q08m41y.jpg" alt="006Fmjmcly1fgcn8dsnw5j318q08m41y"></p>
<p>为了方便计算，加个负对数得到最终的损失函数，这样问题就转化为最小化损失函数了：<img src="/picture/cs224n/lec2/006Fmjmcly1fgcn9ndo8dj316s092jwj.jpg" alt="006Fmjmcly1fgcn9ndo8dj316s092jwj"></p>
<p>接下来就是上几张大家不知道见过多少遍的经典模型图了，以下分别为 CBOW 模型和 SG 模型：<img src="/picture/cs224n/lec2/6cbb8645gw1f5to6e5d9lj216c0qkwhk.jpg" alt="6cbb8645gw1f5to6e5d9lj216c0qkwhk"></p>
<h2 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h2><p>这里参考知乎上<a href="https://www.zhihu.com/question/44832436/answer/266068967" target="_blank" rel="noopener">crystlajj的回答</a>，并偷懒盗用了不少图，以 CBOW 模型的流程为例：</p>
<blockquote>
<ol>
<li>输入层：上下文单词的 onehot. {假设单词向量空间dim为V，上下文单词个数为C}</li>
<li>输出层的 label ：中心词的 onehot。</li>
<li>权重矩阵 W 和 W’：维度分别为 {V*N} 和 {N*V}，并将权重矩阵初始化。<strong>要注意最后得到的输入端的 W 即为单词的词向量表示，词向量的维度为N（每一列表示一个词的词向量）。</strong></li>
<li>所有上下文单词的 onehot 分别乘以共享的输入权重矩阵 W。</li>
<li>将每个单词的 onthot 与 W 相乘所得的 {1*N} 维向量相加求平均作为隐层向量。并将得到乘以输出权重矩阵W’，得到一个 {1*V} 维的向量。</li>
<li>将得到的 {1*V} 维向量经过 softmax 函数处理得到 V-dim 概率分布，概率最大的index所指示的单词为预测出的中心词（target word）。</li>
<li>与true label的onehot做比较，损失函数越小越好。</li>
</ol>
</blockquote>
<p><img src="/picture/cs224n/lec2/v2-0f439e1bb44c71c8e694cc65cb509263_hd.jpg" alt="v2-0f439e1bb44c71c8e694cc65cb509263_hd"></p>
<p>再偷几张图举个例子，让大家跟着走一遍CBOW模型的流程：</p>
<p><img src="/picture/cs224n/lec2/v2-3e75211b3b675f17a232f29fae0982bc_hd.jpg" alt="v2-3e75211b3b675f17a232f29fae0982bc_hd"></p>
<p><img src="/picture/cs224n/lec2/v2-abd3c7d6bc76c01266e8ddd32acfe31a_hd.jpg" alt="v2-abd3c7d6bc76c01266e8ddd32acfe31a_hd"></p>
<p><img src="/picture/cs224n/lec2/v2-66655880a87789eaba5dd6f5c5033e94_hd.jpg" alt="v2-66655880a87789eaba5dd6f5c5033e94_hd"></p>
<p><img src="/picture/cs224n/lec2/v2-5325f4a5d1fbacefd93ccb138b706a69_hd.jpg" alt="v2-5325f4a5d1fbacefd93ccb138b706a69_hd"></p>
<p><img src="/picture/cs224n/lec2/v2-1713450fa2a0f37c8cbcce4ffef04baa_hd.jpg" alt="v2-1713450fa2a0f37c8cbcce4ffef04baa_hd"></p>
<p>以上就是CBOW 模型的流程，这里再附上一张课件中给出的 SG 模型的流程，可以对比一下。</p>
<p><img src="/picture/cs224n/lec2/sg.jpg" alt="sg"></p>
<p>以上就是 Word2Vec 的两种模型的算法流程。其实这种 distributed representation 的思想其实很早就有了，而真正火起来是在 13 年 Mikolov 大神的两篇文章之后。</p>
<p>因为这个方法实际上存在着一些问题。比如训练词向量模型时我们输入端和输出端用到的依旧是 onehot 编码，维度灾难问题仍然存在。如果语料足够大的话，onehot 编码的长度就很大，每轮学习需要更新大量的参数，将会导致训练速度慢到难以接受的地步，如果语料不够充足的话，则很难学到高质量的词向量表达。</p>
<h2 id="Word2Vec-优化"><a href="#Word2Vec-优化" class="headerlink" title="Word2Vec 优化"></a>Word2Vec 优化</h2><p>因而 Mikolov 大神就提出了两种高效的训练方法：</p>
<ol>
<li>Hierarchical softmax</li>
<li>Negative sampling</li>
</ol>
<p>这部分是课程里头没有涉及的内容，并且公式太多太繁杂，在这里只做一个大概的介绍（实在是懒得写了 doge），具体的推导和源码解析可以参考 <a href="https://blog.csdn.net/mytestmy/article/details/26969149" target="_blank" rel="noopener">falao_beiliu</a> 和 <a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">Hankcs</a> 的文章。</p>
<h3 id="Hierarchical-softmax"><a href="#Hierarchical-softmax" class="headerlink" title="Hierarchical softmax"></a>Hierarchical softmax</h3><p>以 CBOW 为例，HS 的方法就是在输出层使用一颗 huffman 树来提升训练效率，模型图如下所示：</p>
<p><img src="/picture/cs224n/lec2/6cbb8645gw1f5wmy4jdnwj214w12a42v.jpg" alt="6cbb8645gw1f5wmy4jdnwj214w12a42v"></p>
<p>Huffman 树在这里就不科普了，HS 方法中的 huffman 树的每个叶子结点表示一个单词，可以被 01 唯一地编码。每个非叶子结点相当于一个神经元。</p>
<p>由于 huffman 树的特点可以使得编码长度最短，这里我们将单词以词频来建立huffman 树，就可以使得常用词的路径较短，非常用词的路径较长。HS 方法就可以将原本查找目标词的多分类问题转化为多个二分类问题，在每一个非叶子结点进行 softmax 运算，根据结果选择向左下走或者向右下走，进而直到走到叶子结点，就可以得知预测的目标词是什么。通过这种方法，我们每次更新的只是走过的路径上的参数，而非全部的参数，可以使得效率大幅度提高。</p>
<p>SG 模型的 CBOW 方法同理，结构示意图如下所示：</p>
<p><img src="/picture/cs224n/lec2/6cbb8645gw1f5wqzg68u0j214a120wij.jpg" alt="6cbb8645gw1f5wqzg68u0j214a120wij"></p>
<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>回想一下之前的算法，我们输出的 label 都是一个词的 one-hot 编码，维度为整个词库的大小，这里的目标词的 index 即为分类任务的正例，其他的词都为负例，也就是说在一个单词数为 N 的词库中，我们的分类任务的正例为 1，负例为 N - 1。正常 N 值会远远大于 1，但是每次更新中，所有的负例的参数权重也会跟着一块更新，在 Mikolov 看来这其实是一个很没有效率的事情，我们大可以把负例减少来提高效率并且对于最后的结果不会产生大的影响。</p>
<p>因此就提出了 NS 方法，顾名思义，NS 即随机挑选出一些负例。采样的算法思想都应该保证频次越高的样本越容易被采集到，因此，NS 算法基本的思路是对于长度为1的线段，根据词语的词频将其公平地分配给每个词语：</p>
<p>$len(w) = \frac{cnt(w)^N}{\sum_{u\epsilon D}cnt(u)^N}$</p>
<p>所以就可以根据词频来分配线段中每个词所占有的长度了。这里 Word2Vec 为了提高效率，用了点小 trick，即将原本线段标上 M 个刻度，这样就不需要浮点数的操作，而是直接使用 0-M 间的整数，经过查表就可以得知该取哪个单词了。</p>
<p>注意这个 len(w) 的计算有包含一个幂计算 (N)，这实际上是一种平滑的策略，可以增大低频词的值，使之更容易被采样到。</p>
<hr>
<p>以上大概就是本节课的内容和扩展，实际上课程里头还包含了一些数学知识的复习和 sgd 算法的推导，觉得过于老生常谈，就不拿出来说了，之后如果有相似的内容也会一样跳过。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol>
<li><a href="https://www.youtube.com/watch?v=ERibwqs9p38&amp;t=0s&amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;index=3" target="_blank" rel="noopener">cs224n lecture2</a></li>
<li><a href="http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf" target="_blank" rel="noopener">cs224n lecture2 slides</a></li>
<li><a href="http://www.hankcs.com/nlp/word-vector-representations-word2vec.html" target="_blank" rel="noopener">cs224n笔记2 词的向量表示</a></li>
<li><a href="http://www.hankcs.com/nlp/word2vec.html" target="_blank" rel="noopener">word2vec原理推导与代码分析</a></li>
<li><a href="https://blog.csdn.net/mytestmy/article/details/26969149" target="_blank" rel="noopener">深度学习word2vec笔记之算法篇</a></li>
</ol>

      
      
        <div class="page-reward">
          <p><a href="javascript:void(0)" onclick="dashangToggle()" class="dashang">赏</a></p>
          <div class="hide_box"></div>
          <div class="shang_box">
            <a class="shang_close" href="javascript:void(0)" onclick="dashangToggle()">×</a>
            <div class="shang_tit">
              <p>Buy Me a Coffee</p>
            </div>
            <div class="shang_payimg">
              <img src="/img/alipayimg.jpg" alt="扫码支持" title="Scan QR Code" />
            </div>
              <div class="pay_explain">Thanks</div>
            <div class="shang_payselect">
              
                <div class="pay_item checked" data-id="alipay">
                  <span class="radiobox"></span>
                  <span class="pay_logo"><img src="/img/alipay.png" alt="Alipay" /></span>
                </div>
              
              
                <div class="pay_item" data-id="wechat">
                  <span class="radiobox"></span>
                  <span class="pay_logo"><img src="/img/weixin.png" alt="Wechat" /></span>
                </div>
              
            </div>
            <div class="shang_info">
              <p>Open <span id="shang_pay_txt">Alipay</span> and Scan QR Code.</p>
            </div>
          </div>
        </div>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/zepto/1.2.0/zepto.min.js"></script>
        <script type="text/javascript">
          $(".pay_item").click(function(){
            $(this).addClass('checked').siblings('.pay_item').removeClass('checked');
            var dataid=$(this).attr('data-id');
            $(".shang_payimg img").attr("src","/img/"+dataid+"img.jpg");
            $("#shang_pay_txt").text(dataid=="alipay"?"Alipay":"Wechat");
          });
          function dashangToggle(){
            
            $(".hide_box").fadeToggle();
            $(".shang_box").fadeToggle();
          }
        </script>
      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>Title:</span><a href="/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/">CS224n学习笔记 Lecture 2 Word Vector Representations word2vec</a></p>
        <p><span>Author:</span><a href="/" title="Hsiao's Blog">Hsiao</a></p>
        <p><span>Date:</span>2018 : 05 : 06 - 02 : 12</p>
        <p><span>Update:</span>2018 : 07 : 24 - 02 : 12</p>
        <p>
            <span>Hyperlink:</span><a class="post-url" href="/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/" title="CS224n学习笔记 Lecture 2 Word Vector Representations word2vec">http://hsiaoyetgun.github.io/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/</a>
            <span class="copy-path" data-clipboard-text="Source text: http://hsiaoyetgun.github.io/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/　　Author: Hsiao" title="Copy"><i class="fa fa-clipboard"></i></span>
            <script src="/js/clipboard.min.js"></script>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>License:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" title="China (CC BY-NC-SA 3.0 CN)" target = "_blank">"Signature-non-commercial-sharing in the same way 3.0"</a> Please keep the original link and author.
        </p>
    </div>



<nav id="article-nav">
  
    <a href="/2018/05/06/CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding
        
      </div>
    </a>
  
  
    <a href="/2018/05/03/CS224n学习笔记-Lecture-1-Introduction-to-NLP-and-Deep-Learning/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">CS224n学习笔记 Lecture 1 Introduction to NLP and Deep Learning</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>

  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">Archives</strong>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#如何表示词的意思"><span class="toc-number">1.</span> <span class="toc-text">如何表示词的意思</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#在计算机中我们该如何得到可用的词意"><span class="toc-number">2.</span> <span class="toc-text">在计算机中我们该如何得到可用的词意</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#从-symbolic-representations-到-distributed-representations"><span class="toc-number">3.</span> <span class="toc-text">从 symbolic representations 到 distributed representations</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Word2Vec"><span class="toc-number">4.</span> <span class="toc-text">Word2Vec</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定义"><span class="toc-number">4.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec-的主要思想"><span class="toc-number">4.2.</span> <span class="toc-text">Word2Vec 的主要思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型流程"><span class="toc-number">4.3.</span> <span class="toc-text">模型流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Word2Vec-优化"><span class="toc-number">4.4.</span> <span class="toc-text">Word2Vec 优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hierarchical-softmax"><span class="toc-number">4.4.1.</span> <span class="toc-text">Hierarchical softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Negative-Sampling"><span class="toc-number">4.4.2.</span> <span class="toc-text">Negative Sampling</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考文献"><span class="toc-number">5.</span> <span class="toc-text">参考文献</span></a></li></ol>
</div>
<input type="button" id="tocButton" value="Hidden"  title="Hidden / Show Archives">

<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script>
    var valueHide = "Hidden";
    var valueShow = "Show";

    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }
    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
        }
    })
    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
    }
</script>





<div class="bdsharebuttonbox">
	<a href="#" class="fx fa-weibo bds_tsina" data-cmd="tsina" title="Share to Weibo"></a>
	<a href="#" class="fx fa-weixin bds_weixin" data-cmd="weixin" title="Share to Wechat"></a>
	<a href="#" class="fx fa-qq bds_sqq" data-cmd="sqq" title="Share to QQ"></a>
	<a href="#" class="fx fa-facebook-official bds_fbook" data-cmd="fbook" title="Share to Facebook"></a>
	<a href="#" class="fx fa-twitter bds_twi" data-cmd="twi" title="Share to Twitter"></a>
	<a href="#" class="fx fa-linkedin bds_linkedin" data-cmd="linkedin" title="Share to linkedin"></a>
	<a href="#" class="fx fa-files-o bds_copy" data-cmd="copy" title="Share hyperlink"></a>
</div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>




    
        <div id="gitments"></div>
<script src="/js/gitment.browser.js"></script>
<script>
    var gitment = new Gitment({
      id: 'Sun May 06 2018 02:12:25 GMT+0800',
      owner: 'HsiaoYetGun',
      repo: 'HsiaoYetGun.github.io',
      oauth: {
        client_id: '7a4d3f0229c8061aa33f',
        client_secret: '6f12ffc30ae1391899a25203c79b322d92fcef07',
      },
    })
    gitment.render('gitments')
</script>
    



    <div class="scroll" id="post-nav-button">
        
            <a href="/2018/05/06/CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding/" title="Prev: CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding">
                <i class="fa fa-angle-left"></i>
            </a>
        
        <a title="Archives"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>
        
            <a href="/2018/05/03/CS224n学习笔记-Lecture-1-Introduction-to-NLP-and-Deep-Learning/" title="Next: CS224n学习笔记 Lecture 1 Introduction to NLP and Deep Learning">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>
    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/07/09/A-review-on-embedding/">A review on embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/06/06/CS224n学习笔记-Lecture-6-Dependency-Parsing/">CS224n学习笔记 Lecture 6 Dependency Parsing</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/09/CS224n学习笔记 Research Highlight3 Bag of Tricks for Efficient Text Classification/">CS224n学习笔记 Research Highlight3 Bag of Tricks for Efficient Text Classification</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/09/CS224n学习笔记-Lecture-5-Backpropagation-and-Project-Advice/">CS224n学习笔记 Lecture 5 Backpropagation and Project Advice</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/08/CS224n学习笔记-Lecture-4-Word-Window-Classification-and-Neural-Network/">CS224n学习笔记 Lecture 4 Word Window Classification and Neural Network</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/CS224n学习笔记-Research-Highlight2-Linear-Algebraic-Structure-of-Word-Senses-with-Applications-to-Polysemy/">CS224n学习笔记 Research Highlight2 Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/07/CS224n学习笔记-Lecture-3-Advanced-Word-Vector-Representations/">CS224n学习笔记 Lecture 3 Advanced Word Vector Representations</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/06/CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding/">CS224n学习笔记 Research Highlight1 A simple but tough-to-beat baseline for sentence embedding</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/06/CS224n学习笔记-Lecture-2-Word-Vector-Representations-word2vec/">CS224n学习笔记 Lecture 2 Word Vector Representations word2vec</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/05/03/CS224n学习笔记-Lecture-1-Introduction-to-NLP-and-Deep-Learning/">CS224n学习笔记 Lecture 1 Introduction to NLP and Deep Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/02/01/Intro/">Intro</a></li></ul>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
    <script>
        $(".post-list").addClass("toc-article");
        $(".post-list-item a").attr("target","_self");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
            }
        })
    </script>



    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2018 Hsiao
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/luuman/hexo-theme-spfk" target="_blank">spfk</a> by spfk
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >Site Visit: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">Page Visit: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    <script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>
<script src="/js/main.js"></script>

    <script>
        $(document).ready(function() {
            var backgroundnum = 24;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>





<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(

            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>